@InProceedings{Kaelbling1993,
  author    = {Kaelbling, L.},
  booktitle = {Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI)},
  title     = {Learning to {Achieve} {Goals}},
  year      = {1993},
  pages     = {1094--1099},
  abstract  = {Temporal di(cid:11)erence methods solve the temporal credit assignment problem for reinforcement learning. An important subproblem of general reinforcement learning is learning to achieve dynamic goals. Although existing temporal di(cid:11)erence methods, such as Q learning, can be applied to this problem, they do not take advantage of its special structure. This paper presents the DG-learning algorithm, which learns e(cid:14)ciently to achieve dynamically changing goals and exhibits good knowledge transfer between goals. In addition, this paper shows how traditional relaxation techniques can be applied to the problem. Finally, experimental results are given that demonstrate the su-periority of DG learning over Q learning in a moderately large, synthetic, non-deterministic domain.},
  annote    = {[TLDR] The DG-learning algorithm is presented, which learns e(cid:14)ciently to achieve dynamically changing goals and exhibits good knowledge transfer between goals and demonstrates the su-periority of DG learning over Q learning in a moderately large, synthetic, non-deterministic domain.},
  file      = {Semantic Scholar Link:Kaelbling1993 - Learning to Achieve Goals.html:URL:https\://www.semanticscholar.org/paper/Learning-to-Achieve-Goals-Kaelbling/6df43f70f383007a946448122b75918e3a9d6682},
  url       = {https://www.semanticscholar.org/paper/Learning-to-Achieve-Goals-Kaelbling/6df43f70f383007a946448122b75918e3a9d6682},
  urldate   = {2024-05-24},
}

@InProceedings{Schulman2015,
  author    = {John Schulman and Sergey Levine and Pieter Abbeel and Michael I. Jordan and Philipp Moritz},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning, {ICML} 2015, Lille, France, 6-11 July 2015},
  title     = {Trust Region Policy Optimization},
  year      = {2015},
  editor    = {Francis R. Bach and David M. Blei},
  pages     = {1889--1897},
  publisher = {JMLR.org},
  series    = {{JMLR} Workshop and Conference Proceedings},
  volume    = {37},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/icml/SchulmanLAJM15.bib},
  file      = {:Schulman2015 - Trust Region Policy Optimization.pdf:PDF},
  groups    = {Entropy-regularized MDPs, Policy gradients},
  timestamp = {Wed, 29 May 2019 08:41:45 +0200},
  url       = {http://proceedings.mlr.press/v37/schulman15.html},
}


@Article{Neu2017,
  author        = {Neu, Gergely and Jonsson, Anders and G{\'{o}}mez, Vicen{\c{c}}},
  journal       = {CoRR},
  title         = {A unified view of entropy-regularized Markov decision processes},
  year          = {2017},
  volume        = {abs/1705.07798},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, https://dblp.org},
  biburl        = {https://dblp.org/rec/journals/corr/NeuJG17.bib},
  eprint        = {1705.07798},
  eprinttype    = {arxiv},
  file          = {:Neu2017 - A Unified View of Entropy Regularized Markov Decision Processes.pdf:PDF},
  groups        = {Average reward, Entropy-regularized MDPs},
  readstatus    = {read},
  timestamp     = {Mon, 13 Aug 2018 16:47:28 +0200},
}


@InProceedings{Todorov2010,
  author     = {Emanuel Todorov},
  booktitle  = {Advances in Neural Information Processing Systems 23: 24th Annual Conference on Neural Information Processing Systems 2010. Proceedings of a meeting held 6-9 December 2010, Vancouver, British Columbia, Canada},
  title      = {Policy gradients in linearly-solvable MDPs},
  year       = {2010},
  editor     = {John D. Lafferty and Christopher K. I. Williams and John Shawe{-}Taylor and Richard S. Zemel and Aron Culotta},
  pages      = {2298--2306},
  publisher  = {Curran Associates, Inc.},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/conf/nips/Todorov10.bib},
  file       = {:Todorov2010 - Policy Gradients in Linearly Solvable MDPs.pdf:PDF},
  groups     = {Entropy-regularized MDPs, Policy gradients},
  readstatus = {read},
  timestamp  = {Mon, 16 May 2022 15:41:51 +0200},
  url        = {https://proceedings.neurips.cc/paper/2010/hash/69421f032498c97020180038fddb8e24-Abstract.html},
}


'
@InProceedings{Todorov2009a,
  author    = {Todorov, Emanuel},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {Compositionality of optimal control laws},
  year      = {2009},
  publisher = {Curran Associates, Inc.},
  volume    = {22},
  abstract  = {We present a theory of compositionality in stochastic optimal control, showing how task-optimal controllers can be constructed from certain primitives. The primitives are themselves feedback controllers pursuing their own agendas. They are mixed in proportion to how much progress they are making towards their agendas and how compatible their agendas are with the present task. The resulting composite control law is provably optimal when the problem belongs to a certain class. This class is rather general and yet has a number of unique properties - one of which is that the Bellman equation can be made linear even for non-linear or discrete dynamics. This gives rise to the compositionality developed here. In the special case of linear dynamics and Gaussian noise our framework yields analytical solutions (i.e. non-linear mixtures of linear-quadratic regulators) without requiring the final cost to be quadratic. More generally, a natural set of control primitives can be constructed by applying SVD to Greens function of the Bellman equation. We illustrate the theory in the context of human arm movements. The ideas of optimality and compositionality are both very prominent in the field of motor control, yet they are hard to reconcile. Our work makes this possible.},
  file      = {:Todorov2009a - Compositionality of Optimal Control Laws.pdf:PDF},
  url       = {https://papers.nips.cc/paper_files/paper/2009/hash/3eb71f6293a2a31f3569e10af6552658-Abstract.html},
  urldate   = {2023-05-26},
}



@Article{Abounadi2001,
  author    = {Abounadi, Jinane and Bertsekas, Dimitrib and Borkar, Vivek S},
  journal   = {SIAM Journal on Control and Optimization},
  title     = {Learning algorithms for Markov decision processes with average cost},
  year      = {2001},
  number    = {3},
  pages     = {681--698},
  volume    = {40},
  publisher = {SIAM},
}


'
@InProceedings{Vaezipoor2021,
  author     = {Vaezipoor, Pashootan and Li, Andrew C. and Icarte, Rodrigo A. Toro and Mcilraith, Sheila A.},
  title      = {{LTL2Action}: {Generalizing} {LTL} {Instructions} for {Multi}-{Task} {RL}},
  year       = {2021},
  month      = jul,
  pages      = {10497--10508},
  publisher  = {PMLR},
  abstract   = {We address the problem of teaching a deep reinforcement learning (RL) agent to follow instructions in multi-task environments. Instructions are expressed in a well-known formal language \{–\} linear temporal logic (LTL) \{–\} and can specify a diversity of complex, temporally extended behaviours, including conditionals and alternative realizations. Our proposed learning approach exploits the compositional syntax and the semantics of LTL, enabling our RL agent to learn task-conditioned policies that generalize to new instructions, not observed during training. To reduce the overhead of learning LTL semantics, we introduce an environment-agnostic LTL pretraining scheme which improves sample-efficiency in downstream environments. Experiments on discrete and continuous domains target combinatorial task sets of up to ∼1039∼1039{\textbackslash}sim10{\textasciicircum}\{39\} unique tasks and demonstrate the strength of our approach in learning to solve (unseen) tasks, given LTL instructions.},
  file       = {:Vaezipoor2021 - LTL2Action_ Generalizing LTL Instructions for Multi Task RL.pdf:PDF},
  issn       = {2640-3498},
  language   = {en},
  readstatus = {read},
  shorttitle = {{LTL2Action}},
  url        = {https://proceedings.mlr.press/v139/vaezipoor21a.html},
  urldate    = {2023-04-13},
}


@inproceedings{Camacho2019,
  title     = {LTL and Beyond: Formal Languages for Reward Function Specification in Reinforcement Learning},
  author    = {Camacho, Alberto and Toro Icarte, Rodrigo and Klassen, Toryn Q. and Valenzano, Richard and McIlraith, Sheila A.},
  booktitle = {International Joint Conference on
               Artificial Intelligence, {IJCAI-19}},
  xpublisher = {International Joint Conferences on Artificial Intelligence Organization},
  pages     = {6065--6073},
  year      = {2019},
  month     = {7},
  doi       = {10.24963/ijcai.2019/840},
  url       = {https://doi.org/10.24963/ijcai.2019/840},
}

@inproceedings{Icarte2018b,
  title={Teaching multiple tasks to an RL agent using LTL},
  author={Toro Icarte, Rodrigo and Klassen, Toryn Q and Valenzano, Richard and McIlraith, Sheila A},
  booktitle={Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems},
  pages={452--461},
  year={2018}
}

@InProceedings{Barreto2017,
  author    = {Andr{\'{e}} Barreto and Will Dabney and R{\'{e}}mi Munos and Jonathan J. Hunt and Tom Schaul and David Silver and Hado van Hasselt},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, {USA}},
  title     = {Successor Features for Transfer in Reinforcement Learning},
  year      = {2017},
  editor    = {Isabelle Guyon and Ulrike von Luxburg and Samy Bengio and Hanna M. Wallach and Rob Fergus and S. V. N. Vishwanathan and Roman Garnett},
  pages     = {4055--4065},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/nips/BarretoDMHSSH17.bib},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/350db081a661525235354dd3e19b8c05-Abstract.html},
}

'
@InProceedings{Alegre2022,
  author     = {Alegre, Lucas Nunes and Bazzan, Ana and Silva, Bruno C. Da},
  title      = {Optimistic {Linear} {Support} and {Successor} {Features} as a {Basis} for {Optimal} {Policy} {Transfer}},
  year       = {2022},
  month      = jun,
  pages      = {394--413},
  publisher  = {PMLR},
  abstract   = {In many real-world applications, reinforcement learning (RL) agents might have to solve multiple tasks, each one typically modeled via a reward function. If reward functions are expressed linearly, and the agent has previously learned a set of policies for different tasks, successor features (SFs) can be exploited to combine such policies and identify reasonable solutions for new problems. However, the identified solutions are not guaranteed to be optimal. We introduce a novel algorithm that addresses this limitation. It allows RL agents to combine existing policies and directly identify optimal policies for arbitrary new problems, without requiring any further interactions with the environment. We first show (under mild assumptions) that the transfer learning problem tackled by SFs is equivalent to the problem of learning to optimize multiple objectives in RL. We then introduce an SF-based extension of the Optimistic Linear Support algorithm to learn a set of policies whose SFs form a convex coverage set. We prove that policies in this set can be combined via generalized policy improvement to construct optimal behaviors for any new linearly-expressible tasks, without requiring any additional training samples. We empirically show that our method outperforms state-of-the-art competing algorithms both in discrete and continuous domains under value function approximation.},
  file       = {:docs/Alegre2022 - Optimistic Linear Support and Successor Features As a Basis for Optimal Policy Transfer.pdf:PDF},
  issn       = {2640-3498},
  language   = {en},
  readstatus = {read},
  url        = {https://proceedings.mlr.press/v162/alegre22a.html},
  urldate    = {2023-04-14},
}

'
@Article{Icarte2022,
  author     = {Icarte, Rodrigo Toro and Klassen, Toryn Q. and Valenzano, Richard and McIlraith, Sheila A.},
  journal    = {Journal of Artificial Intelligence Research},
  title      = {Reward {Machines}: {Exploiting} {Reward} {Function} {Structure} in {Reinforcement} {Learning}},
  year       = {2022},
  issn       = {1076-9757},
  month      = jan,
  pages      = {173--208},
  volume     = {73},
  abstract   = {Reinforcement learning (RL) methods usually treat reward functions as black boxes. As such, these methods must extensively interact with the environment in order to discover rewards and optimal policies. In most RL applications, however, users have to program the reward function and, hence, there is the opportunity to make the reward function visible – to show the reward function’s code to the RL agent so it can exploit the function’s internal structure to learn optimal policies in a more sample efficient manner. In this paper, we show how to accomplish this idea in two steps. First, we propose reward machines, a type of finite state machine that supports the specification of reward functions while exposing reward function structure. We then describe different methodologies to exploit this structure to support learning, including automated reward shaping, task decomposition, and counterfactual reasoning with off-policy learning. Experiments on tabular and continuous domains, across different tasks and RL agents, show the benefits of exploiting reward structure with respect to sample efficiency and the quality of resultant policies. Finally, by virtue of being a form of finite state machine, reward machines have the expressive power of a regular language and as such support loops, sequences and conditionals, as well as the expression of temporally extended properties typical of linear temporal logic and non-Markovian reward specification.},
  copyright  = {Copyright (c)},
  doi        = {10.1613/jair.1.12440},
  file       = {:Icarte2022 - Reward Machines_ Exploiting Reward Function Structure in Reinforcement Learning.pdf:PDF},
  groups     = {Reward machines},
  keywords   = {reinforcement learning, automated reasoning, causality, knowledge representation},
  language   = {en},
  readstatus = {read},
  shorttitle = {Reward {Machines}},
  url        = {https://www.jair.org/index.php/jair/article/view/12440},
  urldate    = {2023-04-06},
}


@InProceedings{Wan2021,
  author    = {Wan, Yi and Naik, Abhishek and Sutton, Richard S},
  booktitle = {Proceedings of the 38th International Conference on Machine Learning},
  title     = {Learning and Planning in Average-Reward Markov Decision Processes},
  year      = {2021},
  editor    = {Meila, Marina and Zhang, Tong},
  month     = {18--24 Jul},
  pages     = {10653--10662},
  publisher = {PMLR},
  series    = {Proceedings of Machine Learning Research},
  volume    = {139},
  abstract  = {We introduce learning and planning algorithms for average-reward MDPs, including 1) the first general proven-convergent off-policy model-free control algorithm without reference states, 2) the first proven-convergent off-policy model-free prediction algorithm, and 3) the first off-policy learning algorithm that converges to the actual value function rather than to the value function plus an offset. All of our algorithms are based on using the temporal-difference error rather than the conventional error when updating the estimate of the average reward. Our proof techniques are a slight generalization of those by Abounadi, Bertsekas, and Borkar (2001). In experiments with an Access-Control Queuing Task, we show some of the difficulties that can arise when using methods that rely on reference states and argue that our new algorithms are significantly easier to use.},
  file      = {:Wan2021 - Learning and Planning in Average Reward Markov Decision Processes.pdf:PDF},
  groups    = {Average reward},
  pdf       = {http://proceedings.mlr.press/v139/wan21a/wan21a.pdf},
  url       = {https://proceedings.mlr.press/v139/wan21a.html},
}

@InProceedings{Infante2022,
  author       = {Infante, Guillermo and Jonsson, Anders and Vicen{\c{c}} G{\'{o}}mez},
  title        = {Globally Optimal Hierarchical Reinforcement Learning for Linearly-Solvable Markov Decision Processes},
  year         = {2022},
  month        = {Jun.},
  number       = {6},
  pages        = {6970-6977},
  volume       = {36},
  abstractnote = {We present a novel approach to hierarchical reinforcement learning for linearly-solvable Markov decision processes. Our approach assumes that the state space is partitioned, and defines subtasks for moving between the partitions. We represent value functions on several levels of abstraction, and use the compositionality of subtasks to estimate the optimal values of the states in each partition. The policy is implicitly defined on these optimal value estimates, rather than being decomposed among the subtasks. As a consequence, our approach can learn the globally optimal policy, and does not suffer from non-stationarities induced by high-level decisions. If several partitions have equivalent dynamics, the subtasks of those partitions can be shared. We show that our approach is significantly more sample efficient than that of a flat learner and similar hierarchical approaches when the set of boundary states is smaller than the entire state space.},
  doi          = {10.1609/aaai.v36i6.20655},
  file         = {:Infante2022 - Globally Optimal Hierarchical Reinforcement Learning for Linearly Solvable Markov Decision Processes.pdf:PDF},
  groups       = {Hierarchical RL},
  journal      = {Proceedings of the AAAI Conference on Artificial Intelligence},
  ranking      = {rank5},
  readstatus   = {read},
  url          = {https://ojs.aaai.org/index.php/AAAI/article/view/20655},
}


@InProceedings{Wan2021a,
  author     = {Yi Wan and Abhishek Naik and Richard S. Sutton},
  booktitle  = {Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual},
  title      = {Average-Reward Learning and Planning with Options},
  year       = {2021},
  editor     = {Marc'Aurelio Ranzato and Alina Beygelzimer and Yann N. Dauphin and Percy Liang and Jennifer Wortman Vaughan},
  pages      = {22758--22769},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/conf/nips/WanNS21.bib},
  file       = {:Wan2021a - Average Reward Learning and Planning with Options.pdf:PDF},
  groups     = {Average reward, Hierarchical RL},
  readstatus = {skimmed},
  timestamp  = {Tue, 03 May 2022 16:20:49 +0200},
  url        = {https://proceedings.neurips.cc/paper/2021/hash/c058f544c737782deacefa532d9add4c-Abstract.html},
}


@Article{Ghavamzadeh2007,
  author  = {Mohammad Ghavamzadeh and Sridhar Mahadevan},
  journal = {Journal of Machine Learning Research},
  title   = {Hierarchical Average Reward Reinforcement Learning},
  year    = {2007},
  number  = {87},
  pages   = {2629--2669},
  volume  = {8},
  file    = {:ghavamzadeh07a.pdf:PDF},
  url     = {http://jmlr.org/papers/v8/ghavamzadeh07a.html},
}


@InProceedings{Fruit2017b,
  author    = {Fruit, Ronan and Pirotta, Matteo and Lazaric, Alessandro and Brunskill, Emma},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {Regret {Minimization} in {MDPs} with {Options} without {Prior} {Knowledge}},
  year      = {2017},
  publisher = {Curran Associates, Inc.},
  volume    = {30},
  abstract  = {The option framework integrates temporal abstraction into the reinforcement learning model through the introduction of macro-actions (i.e., options). Recent works leveraged on the mapping of Markov decision processes (MDPs) with options to semi-MDPs (SMDPs) and introduced SMDP-versions of exploration-exploitation algorithms (e.g., RMAX-SMDP and UCRL-SMDP) to analyze the impact of options on the learning performance. Nonetheless, the PAC-SMDP sample complexity of RMAX-SMDP can hardly be translated into equivalent PAC-MDP theoretical guarantees, while UCRL-SMDP requires prior knowledge of the parameters characterizing the distributions of the cumulative reward and duration of each option, which are hardly available in practice. In this paper, we remove this limitation by combining the SMDP view together with the inner Markov structure of options into a novel algorithm whose regret performance matches UCRL-SMDP's up to an additive regret term. We show scenarios where this term is negligible and the advantage of temporal abstraction is preserved. We also report preliminary empirical result supporting the theoretical findings.},
  file      = {:Fruit2017b - Regret Minimization in MDPs with Options without Prior Knowledge.pdf:PDF},
  url       = {https://papers.nips.cc/paper_files/paper/2017/hash/90599c8fdd2f6e7a03ad173e2f535751-Abstract.html},
  urldate   = {2024-04-24},
}

@InProceedings{Fruit2017,
  author    = {Ronan Fruit and Alessandro Lazaric},
  booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, {AISTATS} 2017, 20-22 April 2017, Fort Lauderdale, FL, {USA}},
  title     = {Exploration-Exploitation in MDPs with Options},
  year      = {2017},
  editor    = {Aarti Singh and Xiaojin (Jerry) Zhu},
  pages     = {576--584},
  publisher = {{PMLR}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {54},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/aistats/FruitL17.bib},
  file      = {:Fruit2017 - Exploration Exploitation in MDPs with Options.pdf:PDF},
  groups    = {Hierarchical RL},
  timestamp = {Wed, 29 May 2019 08:41:44 +0200},
  url       = {http://proceedings.mlr.press/v54/fruit17a.html},
}


@Article{Nachum2019,
  author  = {Nachum, Ofir and Tang, Haoran and Lu, Xingyu and Gu, Shixiang and Lee, Honglak and Levine, Sergey},
  journal = {arXiv preprint arXiv:1909.10618},
  title   = {Why does hierarchy (sometimes) work so well in reinforcement learning?},
  year    = {2019},
}


@InProceedings{Nachum2018,
  author     = {Nachum, Ofir and Gu, Shixiang (Shane) and Lee, Honglak and Levine, Sergey},
  booktitle  = {Advances in {Neural} {Information} {Processing} {Systems}},
  title      = {Data-{Efficient} {Hierarchical} {Reinforcement} {Learning}},
  year       = {2018},
  publisher  = {Curran Associates, Inc.},
  volume     = {31},
  abstract   = {Hierarchical reinforcement learning (HRL) is a promising approach to extend traditional reinforcement learning (RL) methods to solve more complex tasks. Yet, the majority of current HRL methods require careful task-specific design and on-policy training, making them difficult to apply in real-world scenarios. In this paper, we study how we can develop HRL algorithms that are general, in that they do not make onerous additional assumptions beyond standard RL algorithms, and efficient, in the sense that they can be used with modest numbers of interaction samples, making them suitable for real-world problems such as robotic control. For generality, we develop a scheme where lower-level controllers are supervised with goals that are learned and proposed automatically by the higher-level controllers. To address efficiency, we propose to use off-policy experience for both higher- and lower-level training. This poses a considerable challenge, since changes to the lower-level behaviors change the action space for the higher-level policy, and we introduce an off-policy correction to remedy this challenge. This allows us to take advantage of recent advances in off-policy model-free RL to learn both higher and lower-level policies using substantially fewer environment interactions than on-policy algorithms. We find that our resulting HRL agent is generally applicable and highly sample-efficient. Our experiments show that our method can be used to learn highly complex behaviors for simulated robots, such as pushing objects and utilizing them to reach target locations, learning from only a few million samples, equivalent to a few days of real-time interaction. In comparisons with a  number of prior HRL methods, we find that our approach substantially outperforms previous state-of-the-art techniques.},
  file       = {:Nachum2018 - Data Efficient Hierarchical Reinforcement Learning.pdf:PDF},
  groups     = {Herke's recommendations, Hierarchical RL, Feudal Hierarchical RL},
  readstatus = {skimmed},
  url        = {https://papers.nips.cc/paper_files/paper/2018/hash/e6384711491713d29bc63fc5eeb5ba4f-Abstract.html},
  urldate    = {2023-04-03},
}


@InProceedings{Dayan1992,
  author     = {Dayan, Peter and Hinton, Geoffrey E},
  booktitle  = {Advances in {Neural} {Information} {Processing} {Systems}},
  title      = {Feudal {Reinforcement} {Learning}},
  year       = {1992},
  publisher  = {Morgan-Kaufmann},
  volume     = {5},
  abstract   = {One way to speed up reinforcement learning is to enable learning to  happen simultaneously at multiple resolutions in space and time.  This paper shows how to create a Q-Iearning managerial hierarchy  in which high level managers learn how to set tasks to their sub(cid:173) managers who, in turn, learn how to satisfy them.  Sub-managers  need  not initially understand  their managers' commands.  They  simply learn to maximise their reinforcement in the context of the  current command.  We illustrate the system using a simple maze task ..  As the system  learns  how to get around,  satisfying commands at the multiple  levels, it explores more efficiently than standard, flat,  Q-Iearning  and builds a more comprehensive map.},
  file       = {:Dayan1992 - Feudal Reinforcement Learning.pdf:PDF},
  groups     = {Feudal Hierarchical RL},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/1992/hash/d14220ee66aeec73c49038385428ec4c-Abstract.html},
  urldate    = {2023-05-01},
}

@Article{Kolobov2012,
  author   = {Kolobov, Andrey and {Mausam} and Weld, Daniel S.},
  journal  = {Artificial Intelligence},
  title    = {Discovering hidden structure in factored {MDPs}},
  year     = {2012},
  issn     = {0004-3702},
  month    = sep,
  pages    = {19--47},
  volume   = {189},
  abstract = {Markov Decision Processes (MDPs) describe a wide variety of planning scenarios ranging from military operations planning to controlling a Mars rover. However, todayʼs solution techniques scale poorly, limiting MDPsʼ practical applicability. In this work, we propose algorithms that automatically discover and exploit the hidden structure of factored MDPs. Doing so helps solve MDPs faster and with less memory than state-of-the-art techniques. Our algorithms discover two complementary state abstractions — basis functions and nogoods. A basis function is a conjunction of literals; if the conjunction holds true in a state, this guarantees the existence of at least one trajectory to the goal. Conversely, a nogood is a conjunction whose presence implies the non-existence of any such trajectory, meaning the state is a dead end. We compute basis functions by regressing goal descriptions through a determinized version of the MDP. Nogoods are constructed with a novel machine learning algorithm that uses basis functions as training data. Our state abstractions can be leveraged in several ways. We describe three diverse approaches — GOTH, a heuristic function for use in heuristic search algorithms such as RTDP; ReTrASE, an MDP solver that performs modified Bellman backups on basis functions instead of states; and SixthSense, a method to quickly detect dead-end states. In essence, our work integrates ideas from deterministic planning and basis function-based approximation, leading to methods that outperform existing approaches by a wide margin.},
  doi      = {10.1016/j.artint.2012.05.002},
  file     = {ScienceDirect Full Text PDF:https\://www.sciencedirect.com/science/article/pii/S0004370212000598/pdf?md5=be24c76baea6865f771743348bef131a&pid=1-s2.0-S0004370212000598-main.pdf&isDTMRedir=Y:application/pdf},
  keywords = {Markov Decision Process, MDP, Planning under uncertainty, Generalization, Abstraction, Basis function, Nogood, Heuristic, Dead end},
  url      = {https://www.sciencedirect.com/science/article/pii/S0004370212000598},
  urldate  = {2024-05-24},
}


@InProceedings{Strehl2007,
  author    = {Strehl, Alexander L. and Diuk, Carlos and Littman, Michael L.},
  booktitle = {Proceedings of the 22nd national conference on {Artificial} intelligence - {Volume} 1},
  title     = {Efficient structure learning in factored-state {MDPs}},
  year      = {2007},
  address   = {Vancouver, British Columbia, Canada},
  month     = jul,
  pages     = {645--650},
  publisher = {AAAI Press},
  series    = {{AAAI}'07},
  abstract  = {We consider the problem of reinforcement learning in factored-state MDPs in the setting in which learning is conducted in one long trial with no resets allowed. We show how to extend existing efficient algorithms that learn the conditional probability tables of dynamic Bayesian networks (DBNs) given their structure to the case in which DBN structure is not known in advance. Our method learns the DBN structures as part of the reinforcement-learning process and provably provides an efficient learning algorithm when combined with factored Rmax.},
  isbn      = {9781577353232},
  urldate   = {2024-05-24},
}


@InProceedings{Koller2000,
  author    = {Koller, Daphne and Parr, Ronald},
  booktitle = {Proceedings of the {Sixteenth} conference on {Uncertainty} in artificial intelligence},
  title     = {Policy iteration for factored {MDPs}},
  year      = {2000},
  address   = {San Francisco, CA, USA},
  month     = jun,
  pages     = {326--334},
  publisher = {Morgan Kaufmann Publishers Inc.},
  series    = {{UAI}'00},
  abstract  = {Many large MDPs can be represented compactly using a dynamic Bayesian network. Although the structure of the value function does not retain the structure of the process, recent work has suggested that value functions in factored MDPs can often be approximated well using a factored value function: a linear combination of restricted basis functions, each of which refers only to a small subset of variables. An approximate factored value function for a particular policy can be computed using approximate dynamic programming, but this approach (and others) can only produce an approximation relative to a distance metric which is weighted by the stationary distribution of the current policy. This type of weighted projection is ill-suited to policy improvement. We present a new approach to value determination, that uses a simple closed-form computation to compute a least-squares decomposed approximation to the value function for any weights directly. We then use this value determination algorithm as a subroutine in a policy iteration process. We show that, under reasonable restrictions, the policies induced by a factored value function can be compactly represented as a decision list, and can be manipulated efficiently in a policy iteration process. We also present a method for computing error bounds for decomposed value functions using a variableelimination algorithm for function optimization. The complexity of all of our algorithms depends on the factorization of the system dynamics and of the approximate value function.},
  file      = {Full Text PDF:Koller2000 - Policy Iteration for Factored MDPs.pdf:PDF:https\://dl.acm.org/doi/pdf/10.5555/2073946.2073985},
  isbn      = {9781558607095},
  urldate   = {2024-05-24},
}


@InProceedings{Boutilier1995,
  author    = {Boutilier, Craig and Dearden, Richard and Goldszmidt, Mois{\'e}s and others},
  booktitle = {IJCAI},
  title     = {Exploiting structure in policy construction},
  year      = {1995},
  pages     = {1104--1113},
  volume    = {14},
}

@InProceedings{Wen2020,
  author     = {Wen, Zheng and Precup, Doina and Ibrahimi, Morteza and Barreto, Andre and Van Roy, Benjamin and Singh, Satinder},
  booktitle  = {Advances in {Neural} {Information} {Processing} {Systems}},
  title      = {On {Efficiency} in {Hierarchical} {Reinforcement} {Learning}},
  year       = {2020},
  pages      = {6708--6718},
  publisher  = {Curran Associates, Inc.},
  volume     = {33},
  abstract   = {Hierarchical Reinforcement Learning (HRL) approaches promise to provide more efficient solutions to sequential decision making problems, both in terms of statistical as well as computational efficiency. While this has been demonstrated empirically over time in a variety of tasks, theoretical results quantifying the benefits of such methods are still few and far between. In this paper, we discuss the kind of structure in a Markov decision process which gives rise to efficient HRL methods. Specifically, we formalize the intuition that HRL can exploit well repeating "subMDPs", with similar reward and transition structure. We show that, under reasonable assumptions, a model-based Thompson sampling-style HRL algorithm that exploits this structure is statistically efficient, as established through a finite-time regret bound. We also establish conditions under which planning with structure-induced options is near-optimal and computationally efficient.},
  file       = {:Wen2020 - On Efficiency in Hierarchical Reinforcement Learning.pdf:PDF},
  readstatus = {skimmed},
  url        = {https://proceedings.neurips.cc/paper/2020/hash/4a5cfa9281924139db466a8a19291aff-Abstract.html},
  urldate    = {2023-04-20},
}


@InProceedings{Barreto2019,
  author     = {Andr{\'{e}} Barreto and Diana Borsa and Shaobo Hou and Gheorghe Comanici and Eser Ayg{\"{u}}n and Philippe Hamel and Daniel Toyama and Jonathan J. Hunt and Shibl Mourad and David Silver and Doina Precup},
  booktitle  = {Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada},
  title      = {The Option Keyboard: Combining Skills in Reinforcement Learning},
  year       = {2019},
  editor     = {Hanna M. Wallach and Hugo Larochelle and Alina Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and Emily B. Fox and Roman Garnett},
  pages      = {13031--13041},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/conf/nips/BarretoBHCAHTHM19.bib},
  file       = {:Barreto2019 - The Option Keyboard_ Combining Skills in Reinforcement Learning.pdf:PDF},
  groups     = {Hierarchical RL},
  readstatus = {read},
  timestamp  = {Mon, 16 May 2022 15:41:51 +0200},
  url        = {https://proceedings.neurips.cc/paper/2019/hash/251c5ffd6b62cc21c446c963c76cf214-Abstract.html},
}


@InProceedings{Saxe2017,
  author    = {Saxe, Andrew M. and Earle, Adam C. and Rosman, Benjamin},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  title     = {Hierarchy {Through} {Composition} with {Multitask} {LMDPs}},
  year      = {2017},
  month     = jul,
  pages     = {3017--3026},
  publisher = {PMLR},
  abstract  = {Hierarchical architectures are critical to the scalability of reinforcement learning methods. Most current hierarchical frameworks execute actions serially, with macro-actions comprising sequences of primitive actions. We propose a novel alternative to these control hierarchies based on concurrent execution of many actions in parallel. Our scheme exploits the guaranteed concurrent compositionality provided by the linearly solvable Markov decision process (LMDP) framework, which naturally enables a learning agent to draw on several macro-actions simultaneously to solve new tasks. We introduce the Multitask LMDP module, which maintains a parallel distributed representation of tasks and may be stacked to form deep hierarchies abstracted in space and time.},
  file      = {:saxe_hierarchy_2017 - Hierarchy through Composition with Multitask LMDPs.pdf:PDF;:Saxe2017 - Hierarchy through Composition with Multitask LMDPs.pdf:PDF},
  issn      = {2640-3498},
  language  = {en},
  url       = {https://proceedings.mlr.press/v70/saxe17a.html},
  urldate   = {2024-04-24},
}

@Article{Jonsson2016,
  author    = {Jonsson, Anders and Vicen{\c{c}} G{\'{o}}mez},
  journal   = {Proceedings of the International Conference on Automated Planning and Scheduling},
  title     = {Hierarchical {Linearly}-{Solvable} {Markov} {Decision} {Problems}},
  year      = {2016},
  issn      = {2334-0843},
  month     = mar,
  pages     = {193--201},
  volume    = {26},
  abstract  = {We present a hierarchical reinforcement learning framework that formulates each task in the hierarchy as a special type of Markov decision process for which the Bellman equation is linear and has analytical solution. Problems of this type, called linearly-solvable MDPs (LMDPs) have interesting properties that can be exploited in a hierarchical setting, such as efficient learning of the optimal value function or task compositionality. The proposed hierarchical approach can also be seen as a novel alternative to solving LMDPs with large state spaces. We derive a hierarchical version of the so-called Z-learning algorithm that learns different tasks simultaneously and show empirically that it significantly outperforms the state-of-the-art learning methods in two classical HRL domains: the taxi domain and an autonomous guided vehicle task.},
  copyright = {Copyright (c) 2021 Proceedings of the International Conference on Automated Planning and Scheduling},
  doi       = {10.1609/icaps.v26i1.13750},
  file      = {:Jonsson2016 - Hierarchical Linearly Solvable Markov Decision Problems.pdf:PDF},
  language  = {en},
  url       = {https://ojs.aaai.org/index.php/ICAPS/article/view/13750},
  urldate   = {2023-05-31},
}


@InProceedings{NangueTasse2020,
  author    = {Nangue Tasse, Geraud and James, Steven and Rosman, Benjamin},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {A {Boolean} {Task} {Algebra} for {Reinforcement} {Learning}},
  year      = {2020},
  pages     = {9497--9507},
  publisher = {Curran Associates, Inc.},
  volume    = {33},
  abstract  = {The ability to compose learned skills to solve new tasks is an important property for lifelong-learning agents. In this work we formalise the logical composition of tasks as a Boolean algebra. This allows us to formulate new tasks in terms of the negation, disjunction and conjunction of a set of base tasks. We then show that by learning goal-oriented value functions and restricting the transition dynamics of the tasks, an agent can solve these new tasks with no further learning. We prove that by composing these value functions in specific ways, we immediately recover the optimal policies for all tasks expressible under the Boolean algebra. We verify our approach in two domains---including a high-dimensional video game environment requiring function approximation---where an agent first learns a set of base skills, and then composes them to solve a super-exponential number of new tasks.},
  file      = {:NangueTasse2020 - A Boolean Task Algebra for Reinforcement Learning.pdf:PDF},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/6ba3af5d7b2790e73f0de32e5c8c1798-Abstract.html},
  urldate   = {2023-04-21},
}


@InProceedings{Hunt2019,
  author     = {Hunt, Jonathan and Barreto, Andre and Lillicrap, Timothy and Heess, Nicolas},
  title      = {Composing {Entropic} {Policies} using {Divergence} {Correction}},
  year       = {2019},
  month      = may,
  pages      = {2911--2920},
  publisher  = {PMLR},
  abstract   = {Composing skills mastered in one task to solve novel tasks promises dramatic improvements in the data efficiency of reinforcement learning. Here, we analyze two recent works composing behaviors represented in the form of action-value functions and show that they perform poorly in some situations. As part of this analysis, we extend an important generalization of policy improvement to the maximum entropy framework and introduce an algorithm for the practical implementation of successor features in continuous action spaces. Then we propose a novel approach which addresses the failure cases of prior work and, in principle, recovers the optimal policy during transfer. This method works by explicitly learning the (discounted, future) divergence between base policies. We study this approach in the tabular case and on non-trivial continuous control problems with compositional structure and show that it outperforms or matches existing methods across all tasks considered.},
  file       = {:Hunt2019 - Composing Entropic Policies Using Divergence Correction.pdf:PDF},
  issn       = {2640-3498},
  language   = {en},
  readstatus = {read},
  url        = {https://proceedings.mlr.press/v97/hunt19a.html},
  urldate    = {2023-04-19},
}


@InProceedings{Niekerk2019,
  author     = {Benjamin van Niekerk and Steven D. James and Adam Christopher Earle and Benjamin Rosman},
  booktitle  = {Proceedings of the 36th International Conference on Machine Learning, {ICML} 2019, 9-15 June 2019, Long Beach, California, {USA}},
  title      = {Composing Value Functions in Reinforcement Learning},
  year       = {2019},
  editor     = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  pages      = {6401--6409},
  publisher  = {{PMLR}},
  series     = {Proceedings of Machine Learning Research},
  volume     = {97},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/conf/icml/NiekerkJER19.bib},
  file       = {:Niekerk2019 - Composing Value Functions in Reinforcement Learning.pdf:PDF},
  groups     = {Hierarchical RL},
  readstatus = {read},
  timestamp  = {Thu, 27 May 2021 19:27:16 +0200},
  url        = {http://proceedings.mlr.press/v97/van-niekerk19a.html},
}


@InProceedings{Haarnoja2018a,
  author    = {Haarnoja, Tuomas and Pong, Vitchyr and Zhou, Aurick and Dalal, Murtaza and Abbeel, Pieter and Levine, Sergey},
  booktitle = {2018 {IEEE} {International} {Conference} on {Robotics} and {Automation} ({ICRA})},
  title     = {Composable {Deep} {Reinforcement} {Learning} for {Robotic} {Manipulation}},
  year      = {2018},
  month     = may,
  note      = {ISSN: 2577-087X},
  pages     = {6244--6251},
  abstract  = {Model-free deep reinforcement learning has been shown to exhibit good performance in domains ranging from video games to simulated robotic manipulation and locomotion. However, model-free methods are known to perform poorly when the interaction time with the environment is limited, as is the case for most real-world robotic tasks. In this paper, we study how maximum entropy policies trained using soft Q-learning can be applied to real-world robotic manipulation. The application of this method to real-world manipulation is facilitated by two important features of soft Q-learning. First, soft Q-learning can learn multimodal exploration strategies by learning policies represented by expressive energy-based models. Second, we show that policies learned with soft Q-learning can be composed to create new policies, and that the optimality of the resulting policy can be bounded in terms of the divergence between the composed policies. This compositionality provides an especially valuable tool for real-world manipulation, where constructing new policies by composing existing skills can provide a large gain in efficiency over training from scratch. Our experimental evaluation demonstrates that soft Q-learning is substantially more sample efficient than prior model-free deep reinforcement learning methods, and that compositionality can be performed for both simulated and real-world tasks.},
  doi       = {10.1109/ICRA.2018.8460756},
  file      = {IEEE Xplore Full Text PDF:Haarnoja2018a - Composable Deep Reinforcement Learning for Robotic Manipulation.html:URL:https\://ieeexplore.ieee.org/stampPDF/getPDF.jsp?tp=&arnumber=8460756&ref=aHR0cHM6Ly9pZWVleHBsb3JlLmllZWUub3JnL2Fic3RyYWN0L2RvY3VtZW50Lzg0NjA3NTY/Y2FzYV90b2tlbj1QWmFLdjg4WWFjQUFBQUFBOlQ4bWdGWGwxd3JNM0FScWlsTzh6ZkdRMXYtNmE4Tk4xNEROWlFaZ2ZtUGI4S08yTHZGWjVadzBQRkVqYjJZYkwxMWduWkhzUg==},
  issn      = {2577-087X},
  keywords  = {Entropy, Robots, Learning (artificial intelligence), Neural networks, Machine learning, Task analysis, Training},
  url       = {https://ieeexplore.ieee.org/abstract/document/8460756?casa_token=PZaKv88YacAAAAAA:T8mgFXl1wrM3ARqilO8zfGQ1v-6a8NN14DNZQZgfmPb8KO2LvFZ5Zw0PFEjb2YbL11gnZHsR},
  urldate   = {2024-05-24},
}


@Article{Todorov2009,
  author    = {Todorov, Emanuel},
  journal   = {Proceedings of the national academy of sciences},
  title     = {Efficient computation of optimal actions},
  year      = {2009},
  number    = {28},
  pages     = {11478--11483},
  volume    = {106},
  file      = {:Todorov2009 - Efficient Computation of Optimal Actions.pdf:PDF},
  publisher = {National Acad Sciences},
}


@InProceedings{Vieillard2020,
  author    = {Vieillard, Nino and Pietquin, Olivier and Geist, Matthieu},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {Munchausen {Reinforcement} {Learning}},
  year      = {2020},
  pages     = {4235--4246},
  publisher = {Curran Associates, Inc.},
  volume    = {33},
  abstract  = {Bootstrapping is a core mechanism in Reinforcement Learning (RL). Most algorithms, based on temporal differences, replace the true value of a transiting state by their current estimate of this value. Yet, another estimate could be leveraged to bootstrap RL: the current policy. Our core contribution stands in a very simple idea: adding the scaled log-policy to the immediate reward. We show that, by slightly modifying Deep Q-Network (DQN) in that way provides an agent that is competitive with the state-of-the-art Rainbow on Atari games, without making use of distributional RL, n-step returns or prioritized replay. To demonstrate the versatility of this idea, we also use it together with an Implicit Quantile Network (IQN). The resulting agent outperforms Rainbow on Atari, installing a new State of the Art with very little modifications to the original algorithm. To add to this empirical study, we provide strong theoretical insights on what happens under the hood -- implicit Kullback-Leibler regularization and increase of the action-gap.},
  file      = {Full Text PDF:Vieillard2020 - Munchausen Reinforcement Learning.pdf:PDF:https\://proceedings.neurips.cc/paper_files/paper/2020/file/2c6a0bae0f071cbbf0bb3d5b11d90a82-Paper.pdf},
  url       = {https://proceedings.neurips.cc/paper/2020/hash/2c6a0bae0f071cbbf0bb3d5b11d90a82-Abstract.html},
  urldate   = {2024-05-24},
}


@InProceedings{BasSerrano2021,
  author    = {Joan Bas{-}Serrano and Sebastian Curi and Andreas Krause and Gergely Neu},
  booktitle = {The 24th International Conference on Artificial Intelligence and Statistics, {AISTATS} 2021, April 13-15, 2021, Virtual Event},
  title     = {Logistic Q-Learning},
  year      = {2021},
  editor    = {Arindam Banerjee and Kenji Fukumizu},
  pages     = {3610--3618},
  publisher = {{PMLR}},
  series    = {Proceedings of Machine Learning Research},
  volume    = {130},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/conf/aistats/Bas-SerranoC0N21.bib},
  file      = {:BasSerrano2021 - Logistic Q Learning.pdf:PDF},
  timestamp = {Wed, 14 Apr 2021 18:58:38 +0200},
  url       = {http://proceedings.mlr.press/v130/bas-serrano21a.html},
}


@TechReport{Levine2018,
  author     = {Levine, Sergey},
  title      = {Reinforcement {Learning} and {Control} as {Probabilistic} {Inference}: {Tutorial} and {Review}},
  year       = {2018},
  month      = may,
  note       = {arXiv:1805.00909 [cs, stat] type: article},
  abstract   = {The framework of reinforcement learning or optimal control provides a mathematical formalization of intelligent decision making that is powerful and broadly applicable. While the general form of the reinforcement learning problem enables effective reasoning about uncertainty, the connection between reinforcement learning and inference in probabilistic models is not immediately obvious. However, such a connection has considerable value when it comes to algorithm design: formalizing a problem as probabilistic inference in principle allows us to bring to bear a wide array of approximate inference tools, extend the model in flexible and powerful ways, and reason about compositionality and partial observability. In this article, we will discuss how a generalization of the reinforcement learning or optimal control problem, which is sometimes termed maximum entropy reinforcement learning, is equivalent to exact probabilistic inference in the case of deterministic dynamics, and variational inference in the case of stochastic dynamics. We will present a detailed derivation of this framework, overview prior work that has drawn on this and related ideas to propose new reinforcement learning and control algorithms, and describe perspectives on future research.},
  doi        = {10.48550/arXiv.1805.00909},
  file       = {:levine_reinforcement_2018 - Reinforcement Learning and Control As Probabilistic Inference_ Tutorial and Review.pdf:PDF},
  keywords   = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics, Statistics - Machine Learning},
  school     = {arXiv},
  shorttitle = {Reinforcement {Learning} and {Control} as {Probabilistic} {Inference}},
  url        = {http://arxiv.org/abs/1805.00909},
  urldate    = {2024-04-24},
}


@InProceedings{Haarnoja2018,
  author     = {Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  title      = {Soft {Actor}-{Critic}: {Off}-{Policy} {Maximum} {Entropy} {Deep} {Reinforcement} {Learning} with a {Stochastic} {Actor}},
  year       = {2018},
  month      = jul,
  pages      = {1861--1870},
  publisher  = {PMLR},
  abstract   = {Model-free deep reinforcement learning (RL) algorithms have been demonstrated on a range of challenging decision making and control tasks. However, these methods typically suffer from two major challenges: very high sample complexity and brittle convergence properties, which necessitate meticulous hyperparameter tuning. Both of these challenges severely limit the applicability of such methods to complex, real-world domains. In this paper, we propose soft actor-critic, an off-policy actor-critic deep RL algorithm based on the maximum entropy reinforcement learning framework. In this framework, the actor aims to maximize expected reward while also maximizing entropy. That is, to succeed at the task while acting as randomly as possible. Prior deep RL methods based on this framework have been formulated as Q-learning methods. By combining off-policy updates with a stable stochastic actor-critic formulation, our method achieves state-of-the-art performance on a range of continuous control benchmark tasks, outperforming prior on-policy and off-policy methods. Furthermore, we demonstrate that, in contrast to other off-policy algorithms, our approach is very stable, achieving very similar performance across different random seeds.},
  file       = {Full Text PDF:Haarnoja2018 - Soft Actor Critic_ off Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.pdf:PDF:http\://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf;Supplementary PDF:Haarnoja2018 - Soft Actor Critic_ off Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor (1).pdf:PDF:http\://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b-supp.pdf},
  issn       = {2640-3498},
  language   = {en},
  shorttitle = {Soft {Actor}-{Critic}},
  url        = {https://proceedings.mlr.press/v80/haarnoja18b.html},
  urldate    = {2024-05-24},
}



@InProceedings{Silva2009,
  author    = {da Silva, Marco and Durand, Fr{\'e}do and Popovi{\'c}, Jovan},
  booktitle = {{ACM} {SIGGRAPH} 2009 papers},
  title     = {Linear {Bellman} combination for control of character animation},
  year      = {2009},
  address   = {New York, NY, USA},
  month     = jul,
  pages     = {1--10},
  publisher = {Association for Computing Machinery},
  series    = {{SIGGRAPH} '09},
  abstract  = {Controllers are necessary for physically-based synthesis of character animation. However, creating controllers requires either manual tuning or expensive computer optimization. We introduce linear Bellman combination as a method for reusing existing controllers. Given a set of controllers for related tasks, this combination creates a controller that performs a new task. It naturally weights the contribution of each component controller by its relevance to the current state and goal of the system. We demonstrate that linear Bellman combination outperforms naive combination often succeeding where naive combination fails. Furthermore, this combination is provably optimal for a new task if the component controllers are also optimal for related tasks. We demonstrate the applicability of linear Bellman combination to interactive character control of stepping motions and acrobatic maneuvers.},
  doi       = {10.1145/1576246.1531388},
  file      = {Full Text PDF:Silva2009 - Linear Bellman Combination for Control of Character Animation.html:URL:https\://dl.acm.org/doi/pdf/10.1145/1576246.1531388},
  isbn      = {9781605587264},
  keywords  = {optimal control, physically based animation},
  url       = {https://dl.acm.org/doi/10.1145/1576246.1531388},
  urldate   = {2024-05-24},
}

@InProceedings{Mnih2016,
  author    = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  title     = {Asynchronous {Methods} for {Deep} {Reinforcement} {Learning}},
  year      = {2016},
  month     = jun,
  pages     = {1928--1937},
  publisher = {PMLR},
  abstract  = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.},
  file      = {Full Text PDF:Mnih2016 - Asynchronous Methods for Deep Reinforcement Learning.pdf:PDF:http\://proceedings.mlr.press/v48/mniha16.pdf},
  issn      = {1938-7228},
  language  = {en},
  url       = {https://proceedings.mlr.press/v48/mniha16.html},
  urldate   = {2024-05-24},
}


@Article{Kappen2005,
  author    = {Kappen, Hilbert J.},
  journal   = {Physical Review Letters},
  title     = {Linear {Theory} for {Control} of {Nonlinear} {Stochastic} {Systems}},
  year      = {2005},
  month     = nov,
  number    = {20},
  pages     = {200201},
  volume    = {95},
  abstract  = {We address the role of noise and the issue of efficient computation in stochastic optimal control problems. We consider a class of nonlinear control problems that can be formulated as a path integral and where the noise plays the role of temperature. The path integral displays symmetry breaking and there exists a critical noise value that separates regimes where optimal control yields qualitatively different solutions. The path integral can be computed efficiently by Monte Carlo integration or by a Laplace approximation, and can therefore be used to solve high dimensional stochastic control problems.},
  doi       = {10.1103/PhysRevLett.95.200201},
  file      = {Full Text PDF:https\://journals.aps.org/prl/pdf/10.1103/PhysRevLett.95.200201:application/pdf},
  publisher = {American Physical Society},
  url       = {https://link.aps.org/doi/10.1103/PhysRevLett.95.200201},
  urldate   = {2024-05-24},
}


@InCollection{Dvijotham2012,
  author    = {Dvijotham, K. and Todorov, E.},
  booktitle = {Reinforcement {Learning} and {Approximate} {Dynamic} {Programming} for {Feedback} {Control}},
  publisher = {John Wiley \& Sons, Ltd},
  title     = {Linearly {Solvable} {Optimal} {Control}},
  year      = {2012},
  chapter   = {6},
  isbn      = {9781118453988},
  pages     = {119--141},
  abstract  = {The chapter summarizes the recently developed framework of linearly solvable stochastic optimal control. Using an exponential transformation, the (Hamilton-Jacobi) Bellman equation for such problems can be made linear, giving rise to efficient numerical methods. Extensions to game theory are also possible and lead to linear Isaacs equations. The key restriction that makes a stochastic optimal control problem linearly solvable is that the noise and the controls must act in the same subspace. The chapter focuses on discrete-time problems (i.e., Linearly Solvable Markov Decision Processes (LMDPs)), and summarizes related results in continuous time. It briefly introduces the notion of game theoretic control or robust control. The chapter provides a unified treatment of the developments in linearly solvable optimal control. Controlled Vocabulary Terms control systems; numerical analysis; optimal control; stochastic processes},
  copyright = {Copyright © 2013 The Institute of Electrical and Electronics Engineers, Inc.},
  doi       = {10.1002/9781118453988.ch6},
  file      = {Full Text PDF:https\://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/9781118453988.ch6:application/pdf},
  keywords  = {game theoretic control, Linearly Solvable Markov Decision Processes (LMDPs), linearly solvable stochastic optimal control, path-integral control, risk-sensitive control},
  language  = {en},
  url       = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118453988.ch6},
  urldate   = {2024-05-24},
}



@Article{Kappen2012,
  author   = {Kappen, Hilbert J. and Vicen{\c{c}} G{\'{o}}mez and Opper, Manfred},
  journal  = {Machine Learning},
  title    = {Optimal control as a graphical model inference problem},
  year     = {2012},
  issn     = {1573-0565},
  month    = may,
  number   = {2},
  pages    = {159--182},
  volume   = {87},
  abstract = {We reformulate a class of non-linear stochastic optimal control problems introduced by Todorov (in Advances in Neural Information Processing Systems, vol. 19, pp. 1369–1376, 2007) as a Kullback-Leibler (KL) minimization problem. As a result, the optimal control computation reduces to an inference computation and approximate inference methods can be applied to efficiently compute approximate optimal controls. We show how this KL control theory contains the path integral control method as a special case. We provide an example of a block stacking task and a multi-agent cooperative game where we demonstrate how approximate inference can be successfully applied to instances that are too complex for exact computation. We discuss the relation of the KL control approach to other inference approaches to control.},
  doi      = {10.1007/s10994-012-5278-7},
  file     = {:Kappen2012 - Optimal Control As a Graphical Model Inference Problem.pdf:PDF},
  keywords = {Optimal control, Uncontrolled dynamics, Kullback-Leibler divergence, Graphical model, Approximate inference, Cluster variation method, Belief propagation},
  language = {en},
  url      = {https://doi.org/10.1007/s10994-012-5278-7},
  urldate  = {2023-04-29},
}

@Article{Barto2003,
  author   = {Barto, Andrew G. and Mahadevan, Sridhar},
  journal  = {Discrete Event Dynamic Systems},
  title    = {Recent {Advances} in {Hierarchical} {Reinforcement} {Learning}},
  year     = {2003},
  issn     = {1573-7594},
  month    = oct,
  number   = {4},
  pages    = {341--379},
  volume   = {13},
  abstract = {Reinforcement learning is bedeviled by the curse of dimensionality: the number of parameters to be learned grows exponentially with the size of any compact encoding of a state. Recent attempts to combat the curse of dimensionality have turned to principled ways of exploiting temporal abstraction, where decisions are not required at each step, but rather invoke the execution of temporally-extended activities which follow their own policies until termination. This leads naturally to hierarchical control architectures and associated learning algorithms. We review several approaches to temporal abstraction and hierarchical organization that machine learning researchers have recently developed. Common to these approaches is a reliance on the theory of semi-Markov decision processes, which we emphasize in our review. We then discuss extensions of these ideas to concurrent activities, multiagent coordination, and hierarchical memory for addressing partial observability. Concluding remarks address open challenges facing the further development of reinforcement learning in a hierarchical setting.},
  doi      = {10.1023/A:1025696116075},
  file     = {Full Text PDF:Barto2003 - Recent Advances in Hierarchical Reinforcement Learning.html:URL:https\://link.springer.com/content/pdf/10.1023%2Fa%3A1025696116075.pdf},
  keywords = {reinforcement learning, Markov decision processes, semi-Markov decision processes, hierarchy, temporal abstraction},
  language = {en},
  url      = {https://doi.org/10.1023/A:1025696116075},
  urldate  = {2024-05-24},
}


@Book{Ziebart2010,
  author    = {Ziebart, Brian D},
  publisher = {Carnegie Mellon University},
  title     = {Modeling purposeful adaptive behavior with the principle of maximum causal entropy},
  year      = {2010},
}


@Book{Puterman1994,
  author    = {Martin L. Puterman},
  publisher = {Wiley},
  title     = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
  year      = {1994},
  isbn      = {978-0-47161977-2},
  series    = {Wiley Series in Probability and Statistics},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl    = {https://dblp.org/rec/books/wi/Puterman94.bib},
  doi       = {10.1002/9780470316887},
  timestamp = {Mon, 22 Jul 2019 15:00:49 +0200},
  url       = {https://doi.org/10.1002/9780470316887},
}


@InProceedings{Todorov2006,
  author     = {Todorov, Emanuel},
  booktitle  = {Advances in Neural Information Processing Systems},
  title      = {Linearly-solvable Markov decision problems},
  year       = {2006},
  editor     = {B. Sch\"{o}lkopf and J. Platt and T. Hoffman},
  publisher  = {MIT Press},
  volume     = {19},
  file       = {:Todorov2006 - Linearly Solvable Markov Decision Problems.pdf:PDF},
  groups     = {Entropy-regularized MDPs},
  readstatus = {read},
  url        = {https://proceedings.neurips.cc/paper/2006/file/d806ca13ca3449af72a1ea5aedbed26a-Paper.pdf},
}


@InProceedings{Parr1997,
  author    = {Parr, Ronald and Russell, Stuart},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  title     = {Reinforcement {Learning} with {Hierarchies} of {Machines}},
  year      = {1997},
  publisher = {MIT Press},
  volume    = {10},
  file      = {:Parr1997 - Reinforcement Learning with Hierarchies of Machines.pdf:PDF},
  groups    = {Hierarchical RL},
  url       = {https://proceedings.neurips.cc/paper/1997/hash/5ca3e9b122f61f8f06494c97b1afccf3-Abstract.html},
  urldate   = {2023-03-27},
}

@Article{Dietterich2000,
  author    = {Dietterich, T. G.},
  journal   = {Journal of Artificial Intelligence Research},
  title     = {Hierarchical {Reinforcement} {Learning} with the {MAXQ} {Value} {Function} {Decomposition}},
  year      = {2000},
  issn      = {1076-9757},
  month     = nov,
  pages     = {227--303},
  volume    = {13},
  copyright = {Copyright (c)},
  doi       = {10.1613/jair.639},
  file      = {:Dietterich2000 - Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition.pdf:PDF},
  language  = {en},
  url       = {https://www.jair.org/index.php/jair/article/view/10266},
  urldate   = {2023-05-08},
}

@Article{Sutton1999,
  author     = {Richard S. Sutton and Doina Precup and Satinder Singh},
  journal    = {Artif. Intell.},
  title      = {Between MDPs and Semi-MDPs: {A} Framework for Temporal Abstraction in Reinforcement Learning},
  year       = {1999},
  number     = {1-2},
  pages      = {181--211},
  volume     = {112},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/ai/SuttonPS99.bib},
  doi        = {10.1016/S0004-3702(99)00052-1},
  file       = {:Sutton1999 - Between MDPs and Semi MDPs_ a Framework for Temporal Abstraction in Reinforcement Learning.pdf:PDF},
  groups     = {Hierarchical RL},
  readstatus = {read},
  timestamp  = {Tue, 19 Apr 2022 16:03:28 +0200},
}