\section{Introduction}

Most previous work on hierarchical RL considers either the finite-horizon setting or the infinite-horizon setting with discounted rewards.
The average-reward setting is better suited for cyclical tasks characterized by continuous experience.
In the few works on hierarchical RL in the average-reward setting, either the low-level tasks are assumed to be solved beforehand~\citep{Fruit2017,Fruit2017b,Wan2021a} or they % low-level tasks 
have important restrictions that severely reduce their applicability, e.g.~a single initial state~\citep{Ghavamzadeh2007}. It is therefore an open question how to develop algorithms for hierarchical RL in the average-reward setting
%with the average-reward criterion
in order to learn the low-level and high-level tasks simultaneously.



This chapter describes a novel framework for for hierarchical RL in the average-reward setting that simultaneously solves low-level and high-level tasks. Concretely, considering the class of Linearly-solvable Markov Decision Processes (LMDPs)~\citep{Todorov2006}. The method here developed is an extension of the episodic case, described in the previous chapter, to the average-reward setting. Even though the compositionality property for LMDPs or methods of similar flavor have been proposed in RL~\cite{Hunt2019,Niekerk2019,NangueTasse2020} and in combination with Hierarchical RL in the finite-horizon setting~\cite{Jonsson2016,Saxe2017,Infante2022}, adapting this idea to the average-reward setting requires careful analysis and pose a new challenge.



%% VICEN
% LMDPs are a class of restricted MDPs for which the Bellman equation can be exactly transformed into a linear equation. This class of problems plays a key role in the framework of RL as probabilistic inference~\cite{Kappen2012,Levine2018}.
% One of the properties of LMDPs is compositionality: one can compute the solution to a novel task from the solutions to previously solved tasks without learning~\citep{Todorov2009a}. 
%In addition to many other properties, for LMDPs it is possible to compose a solution to a novel task from the solutions to previously solved task without learning~\citep{Todorov2009a}.


%%% PREVIOUS
% Since the Bellman optimality equations are linear for LMDPs, it is possible to compose a solution to a novel task from the solutions to previously solved task without learning~\citep{Todorov2009a}. Compositionality has been previously exploited for hierarchical reinforcement learning for LMDPs in the finite-horizon setting~\citep{Infante2022}, but adapting the idea to the average-reward setting requires careful analysis.

Similarly to the first-exit case and unlike most frameworks for hierarchical RL, this approach does not decompose the policy, only the value function. Hence the agent never chooses a subtask to solve, and instead uses the subtasks to compose the value function of the high-level task. 
This avoids introducing non-stationarity at the higher level when updating the low-level policies.

\section{Contributions}
%This has the advantage of not causing non-stationarity at the higher level when updating the low-level policies. To the best of our knowledge, our work makes the following novel contributions:
\begin{itemize}
  \item Learning low-level and high-level tasks simultaneously in the average-reward setting, without imposing additional restrictions on the low-level tasks.
    \item Two novel algorithms for solving hierarchical RL in the average reward setting: the first one is based on the eigenvector approach used for solving LMDPs. The second is an online variant in which an agent learns simultaneously the low-level and high-level tasks.
%  \item Representing low-level tasks as finite-horizon rather than average-reward decision processes.
    \item Two main theoretical contributions LMDPs: convergence proofs for both differential soft TD-learning for (non-hierarchical) LMDPs and also for the eigenvector approach in the hierarchical case.
%  \item Proving the convergence of average-reward RL in non-hierarchical LMDPs.
  %\item Applying compositionality for reinforcement learning in the average-reward setting.
\end{itemize}

This work is the first that extends the combination of compositionality and hierarchical RL to the average-reward setting.

\section{Related work}
Most research on hierarchical RL formulates problems as a Semi-Markov Decision Process (SMDP) with options~\citep{Sutton1999} or the MAXQ decomposition~\citep{Dietterich2000}.

\citet{Fruit2017} and~\cite{Fruit2017b} propose algorithms for solving SMDPs with options in the average-reward setting, proving that the regret of their algorithms is polynomial in the size of the SMDP components, which may be smaller than the components of the underlying Markov Decision Process (MDP).
\citet{Wan2021a} present a version of differential Q-learning for SMDPs with options in the average-reward setting, proving that differential Q-learning converges to the optimal policy.
However, the above work assumes that the option policies are given prior to learning.
\citet{Ghavamzadeh2007} propose a framework for hierarchical average-reward RL based on the MAXQ decomposition, in which low-level tasks are also modeled as average-reward decision processes.
However, since the distribution over initial states can change as the high-level policy changes, the authors restrict low-level tasks to have a single initial state.

\citet{Infante2022} combine the compositionality of LMDPs with the equivalence of low-level tasks to develop a framework for hierarchical RL in the finite-horizon setting.
In contrast, our framework based on LMDPs can represent the globally optimal policy.

\section{Alternative method for solving an ALMDP}

An alternative method for solving an ALMDP $\cL$ is to transform it to a first-exit LMDP. 
Given a reference state $s^*$ and an original ALMDP $\cL=\langle\cS,\kernel,\cR\rangle$, define a first-exit LMDP $\cL'=\langle\cS\setminus\{s^*\},\{s^*\}, \kernel',\cR',\cJ'\rangle$, where $\kernel'(s'|s)=\kernel(s'|s)$ for all state pairs $(s,s')\in(\cS\setminus\{s^*\})\times\cS$, and $\cJ(s^*)=0$ (implying $z(s^*)=1$). By inspection of~\eqref{eq:boe_z_lmdp} and~\eqref{eq:boe_z_almdp}, the Bellman optimality equation of $\cL'$ is identical to that of $\cL$ if $\cR'(s)=\cR(s)-\rho$. Even though the agent has no prior knowledge of the exponentiated gain $\Gamma = e^{\eta\rho}$, binary search is performed to find $\Gamma$. For a given estimate $\hat\Gamma$ of $\Gamma$, after solving $\cL'$, $\hat \Gamma z(s^*)$ is compared to $e^{\eta\cR(s^*)}\sum_s\kernel(s|s^*)z(s)$. If $\hat \Gamma z(s^*)$ is greater, then $\hat\Gamma$ is too large, else it is too small.

Alternatively, when $\kernel$ and $\cR$ are not known, an estimate $\hat v$ of the optimal value $v$ and an estimate $\hat\rho$ of the optimal gain $\rho$ are kept using \textit{differential soft TD-learning}, similar to differential Q-learning~\citep{Wan2021}. Samples $(s_t, r_t, s_{t+1})$ generated by the estimated policy $\hat\pi$ derived from $\hat v$ as in \eqref{eq:lmdp_optimal_policy} are collected, and the update rules for $\hat v$ and $\hat \rho$ from \eqref{eq:boe_almdp} are as follows:
\begin{align}
    \hat{v}_{t+1}(s_t) &\gets \hat{v}_t(s_t) + \alpha_t \delta_t,\label{eq:main_v_td_update}\\
    \hat{\rho}_{t+1} &\gets \hat{\rho}_t + \lambda \alpha_t \delta_t.\label{eq:main_rho_td_update}
    \end{align}
Here, the TD error $\delta_t$ is given by
\begin{align*}
\delta_t &= r_t - \hat{\rho}_t - \frac 1 \eta \log \frac {\hat{\pi}_t(s_{t+1}|s_t)} {\kernel(s_{t+1}|s_t)} + \hat{v}_t(s_{t+1}) - \hat{v}_t(s_t)\\
 &= r_t - \hat{\rho}_t + \frac 1 \eta \log \sum_{s'\in\cS} \kernel(s'|s_t) e^{\eta \hat{v}_t(s')} - \hat{v}_t(s_t).
\end{align*}
% The learning rates $\alpha_t$ and $\beta_t$ can be chosen independently.
Note that both updates use the same TD error. At any time, the estimates of $\hat z$ and $\widehat\Gamma$ can be retrieved by exponentiating $\hat v$ and $\hat \rho$, respectively.


\begin{theorem}
    Under mild assumptions, differential soft TD-learning in~\eqref{eq:main_v_td_update} and~\eqref{eq:main_rho_td_update} converges to the optimal values of $v$ and $\rho$ in $\cL$. \label{theo:almdps}
\end{theorem}

\begin{proof}[Proof sketch]
The proof is adapted from the proof of convergence of differential Q-learning~\cite{Abounadi2001,Wan2021}, which requires the ALMDP to be communicating (Assumption~\ref{ass:communicating}).
Define a Bellman operator $T$ as
\[
T(v)(s) = \cR(s) + \frac 1 \eta \log \sum_{s'\in\cS} \kernel(s'|s) e^{ \eta v(s') }.
\]
To adapt the previous proof, it is sufficient to show that $T$ is a non-expansion in the max norm, i.e.~$\infnorm{T(x)-T(y)} \leq \infnorm{x - y}$ for each $x, y\in\real^{|\cS|}$, and that $T$ satisfies $T(x + c\mathds{1}) = T(x) + c\mathds{1}$ for each $x\in\real^{|\cS|}$ and constant $c\in\real$, where $\mathds{1}$ is $|\cS|$-dimensional vector of all ones.
For completeness, the full proof appears in Appendix~\ref{proof:theo_almdps}.
\end{proof}

% Simultaneously, we need to keep an estimate $\widehat\Gamma$ of the exponentiated gain $\Gamma$. We can derive update rules for $\hat z$ and $\widehat \Gamma$ from \eqref{eq:boe_z_almdp} as follows:
% \begin{align}
%   \widehat{z}_{t+1}(s_t)  \gets \widehat{z}_{t}(s_{t}) & + \alpha_t \Bigg(\frac{e^{\eta r_t}}{\widehat\Gamma_t}\dfrac{\cP(s_{t+1}\lvert s_t)}{\widehat\pi_t(s_{t+1}\lvert s_t)}  - \hat z_t(s_t)\Bigg),\label{eq:update_z_e}  \\   %\delta^z_t \\
%   \widehat\Gamma_{t+1}    \gets \widehat\Gamma_t       & + \beta_t \Bigg(\frac{e^{\eta r_t}}{\hat z_t(s_t)}\dfrac{\cP(s_{t+1}\lvert s_t)}{\widehat\pi_t(s_{t+1}\lvert s_t)}  - \widehat \Gamma_t\Bigg).\label{eq:update_gamma}
% \end{align}
% The learning rates $\alpha_t$ and $\beta_t$ can be chosen independently.

\begin{comment}
We also notice that we can perform the updates in the logarithmic space. In this case, the update rules are given by
\begin{align}
  \frac{1}{\eta}\log{\hat z_{t+1}(s_t)}    & \gets \frac{1}{\eta}\log{\hat z_t(s_t)} + \alpha_t \delta_t     \\
  \frac{1}{\eta}\log{\widehat\Gamma_{t+1}} & \gets \frac{1}{\eta}\log{\widehat \Gamma_t} + \beta_t \delta_t.
\end{align}
The TD error $\delta_t$ is given by
\begin{align*}
  \delta_t & = \Big(r_t - \frac{1}{\eta}\log{\widehat\Gamma_t} -\frac{1}{\eta}\log{\frac{\hat\pi(s_{t+1}\lvert s_t)}{\cP(s_{t+1}\lvert s_t)}} + \frac{1}{\eta} \log{\hat z_t(s_{t+1})} - \frac{1}{\eta} \log{\hat z_t(s_{t})}\Big) \\
           & = \Big(r_t- \frac{1}{\eta}\log{\widehat\Gamma_t} + \frac{1}{\eta}\log{G\left[\hat z_t\right](s)} - \frac{1}{\eta} \log{\hat z_t(s_{t})}\Big).
\end{align*}
After exponentiation, the update rules can be rewritten as
\begin{align}
  \hat z_{t+1}(s_t)    & \gets \hat z_t(s_t)e^{\eta\alpha_t\delta_t} =  \hat z_t(s_t)^{1-\alpha_t} \Bigg(\frac{e^{\eta r_t}}{\widehat \Gamma_t}G\left[\hat z_t\right](s_t) \Bigg)^{\alpha_t}  \\
  \widehat\Gamma_{t+1} & \gets \widehat\Gamma_t e^{\eta\beta_t\delta_t} =  \widehat\Gamma_t^{1-\beta_t} \Bigg(e^{\eta r_t}\frac{G\left[\hat z_t\right](s_t)}{\hat z_t(s_t)} \Bigg)^{\beta_t}.
  \label{eq:td_gamma}
\end{align}
{\color{blue} A full derivation of these updates rules can be found in Appendix.} \todo{Should we draw a comparison between the two update rules?}
\end{comment}


\begin{figure*}
  \begin{center}
  \input{figures/chapter2/tikz/nrooms.tex}
  \end{center}
  \caption{a) An example $4$-room ALMDP; b) a single subtask with 5 terminal states $G,L,R,T,B$ that is equivalent to all 4 room subtasks. Rooms are numbered 1 through 4, left-to-right, then top-to-bottom, and exit state $1^B$ refers to the exit $B$ of room $1$, etc. \\}
  \label{fig:ex}
\end{figure*}

\section{Hierarchical Average-Reward LMDPs}

This section presents the approach for hierarchical average-reward LMDPs. The idea is to take advantage of the similarity of the value functions in the first-exit and average-reward settings, and use compositionality to compose the value functions of the subtask LMDPs without additional learning.

%\subsubsection{Hierarchical Decomposition}
\subsection{Hierarchical Decomposition}
Consider an ALMDP $\langle\cS,\kernel,\cR\rangle$. 
Similarly to~\citet{Infante2022}, the state space $\cS$ is assumed to be partitioned into subsets $\left\{\cS_i\right\}^L_{i=1}$, with each partition $\cS_i$ inducing
a first-exit LMDP $\cL_i = \langle\cS_i, \cT_i, \kernel_i, \cR_i,\cJ_i\rangle$.
The components of each such subtask $\cL_i$ are defined as follows:
\begin{itemize}
  \item The set of states is $\cS_i$.
  \item The set of terminal states $\cT_i=\{\tau\in\cS\setminus\cS_i: \exists s\in\cS_i, \kernel(\tau|s)>0\}$ contains states not in $\cS_i$ that are reachable in one step from any state inside the partition.
  \item The transition function $\kernel_i$ and reward function $\cR_i$ are projections of $\kernel$ and $\cR-\hat\rho$ onto $\cS_i$, where $\hat\rho$ is a gain estimate.
  \item $\cJ_i$ is defined for each $\tau\in\cT_i$ as $\cJ_i(\tau)=\hat v(\tau)$, where $\hat v$ is a current value estimate (hence $z_i(\tau)=e^{\eta\hat v(\tau)} = \hat z(\tau)$ is defined by a current exponentiated value estimate $\hat z$).
\end{itemize}
The Bellman optimality equations of each subtask $\cL_i$ are given by
\begin{equation}\label{eq:subtask}
  z_i(s) = e^{\eta \cR_i(s)} \sum_{s'} \kernel_i(s'|s) z_i(s') \;\; \forall s\in\cS_i.
\end{equation}
By inspection of the Bellman optimality equations in~\eqref{eq:boe_z_almdp} and~\eqref{eq:subtask}, they are equal if $\cR_i(s)=\cR(s)-\rho$. Thus, if $z_i(\tau)=z(\tau)$ for each $\tau\in\cT_i$ then the solution of the subtask $\cL_i$ corresponds to the optimal solution for each $s\in\cS_i$. However, in general neither $\rho$ nor $z(\tau)$ are known prior to learning and, therefore, estimates $\hat\rho$ and $\hat z(\tau)$ must be used instead. Each subtask $\cL_i$ can be seen as being {\it parameterized\/} on the value estimates $\hat z(\tau)$ for each $\tau\in\cT_j$ and the gain estimate $\hat\rho$. Every time that $\hat z(\tau)$, $\tau\in\cT_i$, and $\hat\rho$ change, a new value estimate for each $s\in\cS_i$ is obtained by solving the subtask for the new parameters.

%\subsection{Subtask compositionality}
\subsection{Subtask Compositionality}
It is impractical to solve each subtask $\cL_i$ every time the estimate $\hat z(\tau)$ changes for $\tau\in\cT_j$. In order to alleviate this, compositionality for LMDPs can be leveraged. The key insight is to build a basis of value functions that can be combined to obtain the solution for the subtasks.

Consider a subtask $\cL_i=\langle\cS_i,\cT_i,\kernel_i,\cR_i,\cJ_i\rangle$ and let $n=|\cT_i|$.  Let $\{\cL_i^1,\ldots,\cL_i^n\}$ be $n$ base LMDPs that are first-exit LMDPs and terminate in $\cT_i$. These base LMDPs only differ from $\cL_i$ in the reward of each terminal state $\tau^k\in\cT_i$. For all $s\in\cS_i$, the reward for each $\cL_i^k$ is by definition $\cR_i(s)=\cR(s)-\hat\rho$ for all $s\in\cS_i$, while at terminal states $\tau\in\cT_i$ let the reward function is $z_i^k(\tau;\hat\rho)=1$ if $\tau=\tau^k$ and $z_i^k(\tau;\hat\rho)=0$ otherwise. Thus, the base LMDPs are parameterized by the gain estimate $\hat\rho$. This is equivalent to setting the reward to $\cJ_i^k(\tau)=0$ if $\tau=\tau_k$ and $\cJ_i^k(\tau)=-\infty$ otherwise. Intuitively, each base LMDP solves the subtask of reaching one specific terminal state $\tau_k\in\cT_i$.

Assume that the solution $z_i^1(\cdot;\rho),\ldots,z_i^n(\cdot;\rho)$ for the base-LMDPs (for the optimal gain $\rho$) is available as well as the optimal value $z(\tau^k)$ of the original ALMDP for each terminal state $\tau^k\in\cT_i$. Then by compositionality could represent the value function of each terminal state can be represented as a weighted combination of the subtasks:
\begin{equation}
  z(\tau) = \sum_{k=1}^n w_k z_i^k(\tau;\rho) =  \sum_{k=1}^n z(\tau^k) z_i^k(\tau;\rho) \;\;\forall\tau\in\cT_i.
  \label{eq:comp_terminal}
\end{equation}

Clearly, the RHS in the previous expression evaluates to $z(\tau)$ since $z(\tau^k) z_i^k(\tau;\rho) = z(\tau)\cdot 1$ when $\tau = \tau^k$, and
$z(\tau^k) z_i^k(\tau;\rho) = z(\tau^k)\cdot 0$ otherwise.

Thanks to compositionality, it is also possible to represent the value function for each subtask state $s\in\cS_i$ as
\begin{equation}
  z(s) = \sum_{k=1}^n z(\tau^k) z_i^k(s;\rho)\;\;\forall s\in\cS_i.
  \label{eq:comp_internal}
\end{equation}
It is signficant to remark that the base LMDPs depend on the gain $\rho$ by the definition of the reward function. This parameter is not known prior to learning. The subtasks in practice are solved for the latest estimate $\hat\rho$ and must be re-learned for every update of this parameter until convergence.
\subsection{Efficiency of the value representation}
 Similar to previous work~\citep{Wen2020,Infante2022} the equivalence of subtasks is exploited to learn more efficiently. Let $\cC=\{\cC_1,\ldots,\cC_C\}$, $C\leq L$, be a set of equivalence classes, i.e.~a partition of the set of subtasks $\{\cL_1,\ldots,\cL_L\}$ such that all subtasks in a given partition are equivalent.
As before, a set of exit states as $\cE=\cup_{i=1}^L\cT_i$ is also defined.
Due to the decomposition, there is no need to keep an explicit value estimate $\hat z(s)$ for every state $s\in\cS$. Instead, it is sufficient to keep a value function for exit states $\hat z_\cE: \cE\rightarrow\real$ and a value function for each base LMDP of each equivalence class. This is enough to represent the value for any state $s\in\cS$ using the compositionality expression in~\eqref{eq:comp_internal}.

Letting $K=\max_{i=1} ^L\lvert\cS_i\rvert$,  $N=\max_{i=1} ^L\lvert\cT_i\rvert$ and $E=\lvert\cE\rvert$, $O(KN)$ values are needed to represent the base LMDPs of a subtask, and the value function can be represented with $O(CKN + E)$ values. The decomposition leads to an efficient representation of the value function whenever $CKN + E \ll \lvert\cS\rvert$. This is achieved when there are few equivalence classes, the size of each subtask is small (in terms of the number of states) and there are relatively few exit states.

  % {\bf Example}: Consider the $4$-room example depicted in Figure~\ref{fig:ex}. Here there is a single equivalence class that generalizes all the rooms. Therefore, a single subclass is represented with $5\times 5$ states. This subtask induces $5$ base-LMDPs with terminal states in $G,L,R,T,B$. Besides, there is a total of 

  %{\bf Example 1:} 
  \emph{Example 1}:
  Figure~\ref{fig:ex}a) shows an example 4-room ALMDP. When reaching the state marked $G$, separate from the room but reachable in one step from the highlighted location, the system transitions to a restart state (top left corner) and receives reward $0$. In all other states the reward is $-1$. The rooms are connected via doorways, so the subtask corresponding to each room has two terminal states in other rooms, plus the terminal state $G$ in the top right room. The 9 exit states in $\cE$ are highlighted and correspond to states next to doorways, plus $G$. Figure~\ref{fig:ex}b) shows a single subtask that is equivalent to all four room subtasks, since the dynamics is shared inside rooms and the set of terminal states is the same. % as those of the subtasks.
  %Hence the number of equivalent subtasks is $C=1$, the number of non-terminal and terminal states of subtasks is $K=25$ and $N=5$, respectively, and the number of exit states is $E=9$.
    %In the 4-room example, t
    There are five base LMDPs with value functions $z^G$, $z^L$, $z^R$, $z^T$ and $z^B$, respectively. Given an initial value estimate $\hat{z}_\cE$ for each exit state in $\cE$, a value estimate of any state in the top left room is given by $\hat{z}(s)=\hat{z}_\cE(1^B) z^B(s) + \hat{z}_\cE(1^R) z^R(s)$, where $\hat{z}_\cE(G)=\hat{z}_\cE(L)=\hat{z}_\cE(T)=0$ is used to indicate that the terminal states $G$, $L$ and $T$ are not reachable in the top left room. 
    The total number of values needed to store the optimal value function is $E+CKN=9+125=134$, and the base LMDPs are faster to learn since they have smaller state space.
    %We need $CKN = 125$ values to store the value functions of the 5 base LMDPs, and $E=9$ values to store the value estimates of all exit states. Although this is more than the 100 states of the original LMDP, if we increase the number of rooms to $X\times Y$, the term $CKN$ is a constant as long as all rooms have equivalent dynamics, and the number of exit states is $E=(2X-1)(2Y-1)$, which is much smaller than the $25XY$ total states. For $10\times 10$ rooms, the value function decomposition requires $486$ values to represent the values of $2{,}500$ states.


\section{Algorithms}
In this section two novel algorithms for solving hierarchical ALMDPs are described. The first is a two-stage eigenvector approach that relies on first solving the subtasks. The second is an online algorithm in which an agent simultaneously learns the subtasks, the gain and the exit values from samples $(s_t, r_t, s_{t+1})$.
Once again it is important to remark that the values for states $s\notin\cE$ are not explicitly represented.

\subsection{Eigenvector approach}

In the episodic case, the base LMDPs are only solved once, and the solutions are then reused to compute the value function $z_\cE$ on exit states (see section~\ref{section:hlmdps_eigenvector_episodic}). However, in the case of ALMDPs, the reward functions of base LMDPs depend on the current gain estimate $\hat\rho$, which is initially unknown. 

% I think the duration part is false!!!

%If we could estimate the expected {\em duration} $d_i(s)$ of each subtask $\cL_i$ from each state $s\in\cS_i$, we could use the duration to adjust the value estimate $\hat z(s)$ of $s$ according to the current gain estimate $\widehat\Gamma$ by a factor $\widehat \Gamma^{d_i(s)}$. The duration can be recursively defined as $d_i(\tau)=0$ for each $\tau\in\cT_i$ and
%\[
%d_i(s) = 1 + \sum_{s'} \pi_i(s'|s) d_i(s').
%\]
%However, even though the subtask policy $\pi_i$ can be expressed in terms of the base LMDP policies $\pi_i^k$, we have been unable to formulate $d_i$ as a composition of the durations $d_i^k$ of base LMDPs, and hence the duration would have to be recomputed each time we resolve a subtask $\cL_i$, which is inefficient.

%We describe in Algorithm~\ref{alg:halmdps_eigenvector}. 
The eigenvector approach for solving hierarchical ALMDPs appears in Algorithm~\ref{alg:halmdps_eigenvector}. The intuition is that in each iteration, first the subtasks are solved for the latest estimate of the exponentiated gain $\widehat\Gamma$. For this, the base LMDPs are solved with~\eqref{eq:subtask} using with the current value of $\hat\rho$. Then~\eqref{eq:comp_internal} is applied, restricted to $\cE$ to obtain an estimate of the value for the exit states. This yields the system of linear equations
\begin{equation}
    {\bf z_\cE} = G_\cE {\bf z_\cE}.\label{eq:eigenvector}
\end{equation}
Here, the matrix $G_\cE\in\real^{\lvert\cE\rvert\times \lvert\cE\rvert}$ contains the optimal values of the base LMDPs and has elements defined as in~\eqref{eq:comp_internal}. The previously introduced idea to transform the ALMDP $\cL$ to a first-exit LMDP $\cL'$ parameterized on the estimated gain $\hat\rho$ is used, and  the optimal exponentiated gain $\Gamma$ is found using binary search. There is a reference state $s^*\in\cS$ (which is by definition an exit state) and is used the test described above to decide how to update the search interval. 
\begin{algorithm}[!b]
  \caption{Eigenvector approach to solving a hierarchical ALMDP.}
  \begin{algorithmic}[1]
    %\Require{An ALMDP $\cL=\langle\cS,\cP,\cR\rangle$, a partition $\cS_1,\ldots,\cS_L$ of $\cS$, inducing a set of exit states $\cE$, reference state $s^\star$.}

    %\Procedure{\textsc{SolveALMDP}}{$\cL,\cS_1,\ldots,\cS_L,\cE,\epsilon,\eta$}
    \State{{\bf Input:} $\cL,\cS_1,\ldots,\cS_L,\cE,\epsilon,\eta$}
    \State $\text{lo}\gets 0$, $\text{hi}\gets 1$
    \While {$\text{hi} - \text{lo} > \epsilon$}
    \State $\widehat\Gamma \gets (\text{hi} + \text{lo}) \mathbin{/} 2$
    %\For{each subtask $\cL_i$}
    %\State Form the matrix $G_i = G_j(\eta,\widehat{\Gamma}_k)$
    %\State Solve the Bellman optimality equation $\widehat{\bf z}_i = G_i\widehat{\bf z}_i^+$
    %\EndFor
    \State Solve base LMDPs $\cL_j^1,\ldots,\cL_j^n$ for each equivalence class $\cC_j$
    \State Form the matrix $G_\cE$ from the optimal value functions
    \State Solve the system of equations  ${\bf \hat z_\cE} = G_\cE {\bf\hat z_\cE}$
    \If {$ \widehat\Gamma \hat z_\cE(s^*) > e^{\eta \cR(s^*)} \sum_{s\in\cS} \kernel(s\lvert s^*) \hat z_\cE(s)$}
    \State $\text{hi}\gets \widehat\Gamma$
    \Else \State $\text{lo}\gets \widehat\Gamma$
    \EndIf
    \vspace*{3pt}
    \EndWhile
    \Return value functions of all base LMDPs, ${\bf \hat z_\cE}$
    %\EndProcedure
  \end{algorithmic}
  \label{alg:halmdps_eigenvector}
\end{algorithm}

\begin{theorem}\label{thm:converge}
    Algorithm~\ref{alg:halmdps_eigenvector} converges to the optimal value function $z$ of $\cL$ as $\epsilon\to 0$.
\end{theorem}
% The proof of Theorem~\ref{thm:converge} appears in Appendix~\ref{proof:theo_h}.

First note that the optimal value function $z$ of $\cL$ exists and is unique due to Assumption~\ref{ass:communicating}. Due to the equivalence between $\cL$ and the corresponding first-exit LMDP $\cL'$, this implies that $\cL'$ has a unique solution $z(\cdot;\rho)$ when the estimated gain $\hat\rho$ equals $\rho$, and that this solution equals $z(\cdot;\rho)=z$, the optimal solution to $\cL$.

\begin{lemma}\label{lemma:monotonicity}
     Given a first-exit LMDP $\cL'$ parameterized on $\hat\rho$, the optimal value $z(s;\hat\rho)$ of each non-terminal state $s\in\cS$ is strictly monotonically decreasing in $\hat\rho$.
\end{lemma}

\begin{proof}
Strict monotonicity requires that there exists $\varepsilon>0$ such that $\linebreak{z(s;\hat\rho - \varepsilon) > z(s;\hat\rho) > z(s;\hat\rho+\varepsilon)}$ when $\varepsilon\rightarrow 0$. The first inequality is proven by induction; the second is analogous. The base case is given by the terminal states $\tau\in\cT$, for which $z(\tau;\hat\rho - \varepsilon) = z(\tau;\hat\rho)$. The inductive case is given by
\begin{align*}
    z(s;\hat\rho-\varepsilon) &= e^{\eta(\cR(s) - (\hat\rho -\varepsilon))}\sum_{s'\in\cS}\kernel(s'\lvert s) z(s';\hat\rho- \varepsilon)\\
      &\geq e^{\eta\varepsilon} e^{\eta(\cR(s) - \rho))}\sum_{s'\in\cS}\kernel(s'\lvert s) z(s';\hat\rho)\\
      &= e^{\eta\varepsilon} z(s;\hat\rho) > z(s;\hat\rho).
\end{align*}
This concludes the proof.
\end{proof}

As a consequence of Lemma~\ref{lemma:monotonicity}, $\cL'$ has a unique solution $z(\cdot,\hat\rho)$ for each $\hat\rho\geq\rho$, since the values $z(\cdot,\hat\rho)$ decrease as $\hat\rho$ increases. In contrast, there may be values of $\hat\rho>\rho$ for which power iteration does not converge.

\begin{restatable}{lemma}{optimality}
Given a subtask $\cL_i$, if the optimal value of each terminal state $\tau\in\cT_i$ equals its optimal value in $\cL$, i.e. $z_i(\tau) = z(\tau)$, and the optimal gain $\rho$ in $\cL$ is known, then the optimal value of each non-terminal state $s\in\cS_i$ is unique and equals $z_i(s)=z(s)$.
    \label{lemma:optimality}
\end{restatable}

\begin{proof} Since $\cR_i$ and $\kernel_i$ are restrictions of $\cR-\rho$ and $\kernel$, respectively, to $\cS_i$, then
\begin{align*}
    z_i(s) &= e^{\eta\cR_i(s)}\sum_{s'}\kernel_i(s'\lvert s) z_i(s') = e^{\eta(\cR(s) - \rho)}\sum_{s'}\kernel(s'\lvert s) z_i(s'), \nonumber
\end{align*}
which is the same Bellman equation as for $z(s)$. Assuming that $z_i(\tau) = z(\tau)$ for each $\tau\in\cT$, directly yields that $z_i(s)=z(s)$ for each non-terminal state $s\in\cS_i$.
\end{proof}

\begin{corollary}\label{cor:uniqueness}
    If the optimal gain $\rho$ in $\cL$ is known, each base LMDP $\cL_j^k$ has a unique solution $z_j^k(\cdot;\rho)$.
\end{corollary}

\begin{proof} From \eqref{eq:comp_internal}, the optimal values of subtask states satisfy
\[
  z(s) = \sum_{k=1}^n z(\tau^k) z_i^k(s;\rho)\;\;\forall s\in\cS_i.
\]
Due to Lemma~\ref{lemma:optimality}, the optimal value $z(s)$ is unique, which is only possible if $z_i^k(s;\rho)$ is unique for each $\tau^k\in\cT_i$.
\end{proof}

Combined with Lemma~\ref{lemma:monotonicity}, Corollary \ref{cor:uniqueness} implies that each base LMDP $\cL_j^k$ has a unique solution $z_j^k(\cdot;\hat\rho)$ whenever $\hat\rho\geq\rho$.

\begin{lemma}
For $\hat\rho\geq\rho$, the equation ${\bf \hat z_\cE} = G_\cE {\bf\hat z_\cE}$ has a unique solution that equals $z_\cE(\tau)=z(\tau;\hat\rho)$ for each exit $\tau\in\cE$, where $z(\cdot;\hat\rho)$ is the unique value of the first-exit LMDP $\cL'$ for $\hat\rho$.
\end{lemma}

\begin{proof}
At convergence, due to \eqref{eq:comp_internal} it has to hold for each non-terminal exit $\tau\in\cE$ that
\[
  z_\cE(\tau) = \sum_{k=1}^n z_\cE(\tau^k) z_i^k(s;\hat\rho)\;\;\forall s\in\cS_i,
\]
where each $\tau^k$ is also an exit and $z_i^k(s;\hat\rho)$ is well-defined and unique since $\hat\rho\geq\rho$. This equation is satisfied when $z_\cE(\tau)=z(\tau;\hat\rho)$ for each exit. Since $z(\cdot;\hat\rho)$ is unique, this is the only solution.
\end{proof}

Theorem~\ref{thm:converge} can be now proven. When $\hat\rho\geq\rho$ (or equivalently, $\widehat\Gamma\geq\Gamma$), each base LMDP has a unique solution, and $z_\cE$ is also unique. Moreover, when $\hat\rho>\rho$, the condition on line 8 is true, which causes binary search to discard all values greater than $\hat\rho$. If the base LMDPs or $z_\cE$ do not have a unique solution, it means that $\hat\rho$ is too small, and hence all values less than $\hat\rho$ can be discarded. Since the solution $z(\cdot;\hat\rho)$ is monotonically decreasing in $\hat\rho$, binary search is guaranteed to find the optimal gain $\rho$ within a factor of $\epsilon$.

\begin{figure}[!ht]
  \centering
  \includegraphics*[width=0.7\textwidth]{figures/chapter2/eigenvectors/lmdp-nroom-1.pdf}
  \caption{Results in N-room when varying the number of rooms and the size of the rooms.}
  \label{fig:nrooms}
 \end{figure}

\subsection{Online algorithm}
 In the online case (see Algorithm~\ref{alg:halmdps_online}), the agent keeps an estimate of the exponentiated gain $\widehat\Gamma=e^{\eta\hat\rho}$ which is updated every timestep. It also keeps estimates of the value functions of the base LMDPs $\hat z_i^1(\cdot;\hat\rho),\ldots,\hat z_i^n(\cdot;\hat\rho)$ for each equivalence class $\cC_i$, and estimates of the value function on exit states $\hat z_\cE$.  All the base LMDPs of the same equivalence class can be updated with the same sample using intra-task learning with the appropriate {\it importance sampling weights\/}~\citep{Jonsson2016}. For the estimates of the exit states, they are only updated upon visitation of such states. In that case, the compositionality expression in~\eqref{eq:comp_internal} is used to derive the following update:
\begin{equation}
  \hat z_\cE(s)\gets (1-\alpha_\ell) \hat z_\cE(s) + (1-\alpha_\ell) \sum_{k=1}^n \hat z_\cE(\tau^k) \hat z_i^k(s;\rho).
  \label{eq:td_update_exit}
\end{equation}
Here, $\alpha_\ell$ is the learning rate. Each of the learned components (i.e., gain, base LMDPs and exit state value estimates) maintain independent learning rates.

\begin{algorithm}[!htpb]
  \caption{Online algorithm.}
  \begin{algorithmic}[1]
    \State{{\bf Input:} $\cL,\cS_1,\ldots,\cS_L,\cE,s_0,\epsilon,\eta$}
    \State $t \gets 0$, $\widehat{\Gamma} \gets 1$
    \While {{\it not terminate}}
    \State Observe $(s_{t}, r_{t}, s_{t+1})\sim\hat\pi_t$
    \State Update $\hat z_j^1(\cdot;\hat\rho),\ldots,\hat z_j^n(\cdot;\hat\rho)$ using Equation~\eqref{eq:main_v_td_update}
    \State Compute $\hat z(s_t)$ and $\sum_{s'}\kernel(s'|s_t)\hat z(s')$ using Equation \eqref{eq:comp_internal}
    \State Update $\widehat\Gamma$ using Equation~\eqref{eq:main_rho_td_update}
    \If{$s_t\in\cE$}
    \State Update $\hat z_\cE(s_t)$ using Equation~\eqref{eq:td_update_exit}
    \EndIf
    \EndWhile
  \end{algorithmic}
  \label{alg:halmdps_online}
\end{algorithm}

\section{Experiments}

Algorithm~\ref{alg:halmdps_online} is compared against differential soft TD-learning in the flat representation of the ALMDP. Similarly to the episodic case, the metric used for the experiments Mean Absolute Error (MAE) between the estimated value function $\hat z$ and the true optimal value function $z$. For each algorithm, the results are averaged over five seeds. In addition to the average, the standard deviation is also reported. The learning rates have been optimized independently for each of the instances. The two episodic benchmark tasks~\citep{Infante2022} have been adapted by transforming them into infinite-horizon tasks as follows:

  \subsection{N-room domain}
 As in the episodic case, there are some `goal' states with high reward (i.e.~0). When the agent enters a goal state, the next action causes it to receive the given reward and transition to a {\it restart\/} location. The number of rooms as well as the size of the rooms are varied to obtain different instances of the problem.

  \begin{sidewaysfigure}
  \centering
  \includegraphics*[width=0.32\textwidth]{figures/chapter2/online/nrooms_3_3.pdf}
  \includegraphics*[width=0.32\textwidth]{figures/chapter2/online/nrooms_5_5.pdf}
  \includegraphics*[width=0.32\textwidth]{figures/chapter2/online/nrooms_8_8.pdf}

  \includegraphics*[width=0.32\textwidth]{figures/chapter2/online/nrooms_3_3_subtasks.pdf}
  \includegraphics*[width=0.32\textwidth]{figures/chapter2/online/nrooms_5_5_subtasks.pdf}
  \includegraphics*[width=0.32\textwidth]{figures/chapter2/online/nrooms_8_8_subtasks.pdf}

  \includegraphics*[width=0.32\textwidth]{figures/chapter2/online/nrooms_3_3_gammas.pdf}
  \includegraphics*[width=0.32\textwidth]{figures/chapter2/online/nrooms_5_5_gammas.pdf}
  \includegraphics*[width=0.32\textwidth]{figures/chapter2/online/nrooms_8_8_gammas.pdf}


  \caption{Results in N-room when varying the number of rooms and the size of the rooms.}
  \label{fig:halmdps_nrooms}
  \end{sidewaysfigure}
  


  \subsection{Taxi domain}
  In this variant of the original domain~\citep{Dietterich2000}, once the passenger has been dropped off, the system transitions to a state in which the driver is in the last drop-off location, but a new passenger appears randomly at another location. Hence the driver immediately receives a new assignment after having successfully completed the previous one.
  \begin{figure}[!hb]
    \begin{center}
        \includegraphics*[width=0.49\textwidth]{figures/chapter2/online/taxi_5.pdf}
        \includegraphics*[width=0.49\textwidth]{figures/chapter2/online/taxi_8.pdf}
        % \includegraphics*[width=0.49\textwidth]{figures/chapter2/online/taxi_5_subtasks.pdf}
        % \includegraphics*[width=0.49\textwidth]{figures/chapter2/online/taxi_8_subtasks.pdf}
        \caption{Results for $5 \times 5$ (top) and $8 \times 8$ (bottom) grids of the Taxi domain.}
          \label{fig:halmdps_taxi}
    \end{center}
  \end{figure}
  %In this case, the adaptation to an infinite-horizon task is more natural. Once the passenger has been dropped off, the system transitions to a state in which the driver is in the last dropoff location, but a new passenger appears at another location. %Hence the driver immediately receives a new assignment after having successfully completed the previous one.

\subsection{Results}
Figures~\ref{fig:halmdps_nrooms} and~\ref{fig:halmdps_taxi} show the results. Algorithm~\ref{alg:halmdps_online} is able to speed up 
%The results (Figures~\ref{fig:nrooms} and~\ref{fig:taxi}) show that our algorithm is able to speed up 
learning and converges to the optimal solution faster than flat average-reward reinforcement learning (note the log scale). This is in line with previous results for the episodic case~\citep{Infante2022}. The difference in the error scale in the figures is due to the initialization of the base LMDPs. The average reward setting poses an extra challenge since the `sparsity' of the reward can make the estimates of the gain oscillate. This ultimately has an impact on the estimates of the base LMDPs and the value estimates of the exit states, and it is likely the reason why in Figure~\ref{fig:halmdps_taxi} the error increases before decreasing down to zero.


% \begin{figure*}[!bt]
%   \centering
%   \includegraphics*[width=0.32\textwidth]{pictures/nrooms_3_3-1.png}
%   \includegraphics*[width=0.32\textwidth]{pictures/nrooms_5_5-1.png}
%   \includegraphics*[width=0.32\textwidth]{pictures/nrooms_8_8-1.png}
%   \caption{ Results in N-room when varying the number of rooms and the size of the rooms.}
%   \label{fig:nrooms}
% \end{figure*}

% \begin{figure}[!htp]
%   \centering
%   \includegraphics*[width=0.33\textwidth]{pictures/taxi_5-1.png}
%   \includegraphics*[width=0.33\textwidth]{pictures/taxi_10-1.png}
%   \caption{Results for $5 \times 5$ (left) and $10 \times 10$ (right) grids of the Taxi domain.}
%   \label{fig:taxi}

% \end{figure}


\section{Conclusion}

% In this paper we present a novel framework for hierarchical average-reward reinforcement learning which makes it possible to learn the low-level and high-level tasks simultaneously. 
% We propose an eigenvector approach and an online algorithm for solving problems in our framework, and show that the former converges to the optimal value function. As a by-product of our analysis, we also provide a convergence theorem in the non-hierarchical case for average-reward LMDPs, which to the extent of our knowledge, was not previously done. In the future we would like to prove convergence also for the proposed online algorithm.
