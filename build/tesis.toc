\babel@toc {english}{}\relax 
\babel@toc {catalan}{}\relax 
\contentsline {chapter}{{ Acknowledgements}}{v}{chapter*.1}%
\babel@toc {english}{}\relax 
\contentsline {chapter}{List of figures}{xvi}{chapter*.4}%
\contentsline {chapter}{List of tables}{xvii}{chapter*.5}%
\contentsline {chapter}{\numberline {1}{{ Introduction}}}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Thesis structure}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}Contributions}{2}{section.1.2}%
\contentsline {chapter}{\numberline {2}{{ Background}}}{3}{chapter.2}%
\contentsline {section}{\numberline {2.1}Markov decision Process and reinforcement learning}{4}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}Policies}{4}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}Optimality criteria}{4}{subsection.2.1.2}%
\contentsline {subsection}{\numberline {2.1.3}Value functions}{4}{subsection.2.1.3}%
\contentsline {subsection}{\numberline {2.1.4}Dynamic programming}{4}{subsection.2.1.4}%
\contentsline {subsection}{\numberline {2.1.5}Function approximation}{4}{subsection.2.1.5}%
\contentsline {section}{\numberline {2.2}Successor features}{4}{section.2.2}%
\contentsline {section}{\numberline {2.3}Linearly-solvable Markov decision processes}{5}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}First-exit linearly-solvable Markov decision processes}{6}{subsection.2.3.1}%
\contentsline {subsubsection}{Solving a first-exit LMDP}{8}{section*.6}%
\contentsline {subsubsection}{Compositionality}{9}{section*.7}%
\contentsline {subsection}{\numberline {2.3.2}Average-reward linearly-solvable Markov decision processes}{9}{subsection.2.3.2}%
\contentsline {subsubsection}{Solving an ALMDP}{10}{section*.9}%
\contentsline {subsection}{\numberline {2.3.3}Function approximation in LMDPs}{11}{subsection.2.3.3}%
\contentsline {section}{\numberline {2.4}Hierarchical Reinforcement Learning}{12}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}Optimality of HRL algorithms}{12}{subsection.2.4.1}%
\contentsline {subsection}{\numberline {2.4.2}The options framework}{12}{subsection.2.4.2}%
\contentsline {section}{\numberline {2.5}Non-Markovian task specification}{12}{section.2.5}%
\contentsline {chapter}{\numberline {3}{{ Globally Optimal Hierarchical Reinforcement Learning for Linearly-solvable Markov Decision Processes}}}{13}{chapter.3}%
\contentsline {section}{\numberline {3.1}Introduction}{13}{section.3.1}%
\contentsline {section}{\numberline {3.2}Contributions}{14}{section.3.2}%
\contentsline {section}{\numberline {3.3}Related Work}{14}{section.3.3}%
\contentsline {section}{\numberline {3.4}Hierarchical LMDPs}{16}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Hierarchical Decomposition}{16}{subsection.3.4.1}%
\contentsline {paragraph}{Example 1:}{18}{section*.11}%
\contentsline {subsection}{\numberline {3.4.2}Subtask Compositionality}{18}{subsection.3.4.2}%
\contentsline {paragraph}{Example 1:}{19}{section*.12}%
\contentsline {paragraph}{Example 2:}{20}{section*.14}%
\contentsline {subsection}{\numberline {3.4.3}Eigenvector Approach}{21}{subsection.3.4.3}%
\contentsline {paragraph}{Example 1:}{21}{section*.15}%
\contentsline {subsection}{\numberline {3.4.4}Online and Intra-task Learning}{21}{subsection.3.4.4}%
\contentsline {subsection}{\numberline {3.4.5}Analysis}{23}{subsection.3.4.5}%
\contentsline {section}{\numberline {3.5}Experiments}{24}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1}N-room domain.}{25}{subsection.3.5.1}%
\contentsline {subsection}{\numberline {3.5.2}Taxi Domain.}{26}{subsection.3.5.2}%
\contentsline {section}{\numberline {3.6}Discussion and Conclusion}{26}{section.3.6}%
\contentsline {chapter}{\numberline {4}{{ Hierarchical Average-reward Linearly-solvable Markov Decision Pocesses}}}{29}{chapter.4}%
\contentsline {section}{\numberline {4.1}Introduction}{29}{section.4.1}%
\contentsline {section}{\numberline {4.2}Contributions}{30}{section.4.2}%
\contentsline {section}{\numberline {4.3}Related work}{30}{section.4.3}%
\contentsline {section}{\numberline {4.4}Alternative method for solving an ALMDP}{31}{section.4.4}%
\contentsline {section}{\numberline {4.5}Hierarchical Average-Reward LMDPs}{32}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}Hierarchical Decomposition}{33}{subsection.4.5.1}%
\contentsline {subsection}{\numberline {4.5.2}Subtask Compositionality}{33}{subsection.4.5.2}%
\contentsline {subsection}{\numberline {4.5.3}Efficiency of the value representation}{34}{subsection.4.5.3}%
\contentsline {paragraph}{Example 1}{35}{section*.19}%
\contentsline {section}{\numberline {4.6}Algorithms}{35}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}Eigenvector approach}{35}{subsection.4.6.1}%
\contentsline {subsection}{\numberline {4.6.2}Online algorithm}{38}{subsection.4.6.2}%
\contentsline {section}{\numberline {4.7}Experiments}{39}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1}N-room domain}{40}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2}Taxi domain}{40}{subsection.4.7.2}%
\contentsline {subsection}{\numberline {4.7.3}Results}{40}{subsection.4.7.3}%
\contentsline {section}{\numberline {4.8}Conclusion}{42}{section.4.8}%
\contentsline {chapter}{\numberline {5}{{ Compositionality with Successor Features via Policy Basis}}}{45}{chapter.5}%
\contentsline {section}{\numberline {5.1}Introduction}{45}{section.5.1}%
\contentsline {section}{\numberline {5.2}Contributions}{46}{section.5.2}%
\contentsline {section}{\numberline {5.3}Preliminaries}{47}{section.5.3}%
\contentsline {paragraph}{Propositional Logic}{47}{section*.23}%
\contentsline {paragraph}{Finite State Automaton}{47}{section*.24}%
\contentsline {paragraph}{Feature vectors}{47}{section*.26}%
\contentsline {paragraph}{Policy basis and convex coverage set}{48}{section*.27}%
\contentsline {section}{\numberline {5.4}Using Successor Features to Solve non-Markovian Reward Specifications}{49}{section.5.4}%
\contentsline {paragraph}{Example}{50}{section*.28}%
\contentsline {subsection}{\numberline {5.4.1}Algorithm}{50}{subsection.5.4.1}%
\contentsline {subsection}{\numberline {5.4.2}Analysis}{51}{subsection.5.4.2}%
\contentsline {section}{\numberline {5.5}Experiments}{52}{section.5.5}%
\contentsline {subsection}{\numberline {5.5.1}Environments and tasks}{53}{subsection.5.5.1}%
\contentsline {paragraph}{Office}{53}{section*.30}%
\contentsline {paragraph}{Delivery}{53}{section*.31}%
\contentsline {subsection}{\numberline {5.5.2}Baselines}{54}{subsection.5.5.2}%
\contentsline {subsection}{\numberline {5.5.3}Results}{55}{subsection.5.5.3}%
\contentsline {paragraph}{A motivating example}{55}{section*.34}%
\contentsline {paragraph}{Learning}{56}{section*.35}%
\contentsline {paragraph}{Planning}{57}{section*.36}%
\contentsline {section}{\numberline {5.6}Related Work}{57}{section.5.6}%
\contentsline {section}{\numberline {5.7}Discussion and Conclusion}{59}{section.5.7}%
\contentsline {chapter}{\numberline {A}{{ Proof Theorem~\ref {theo:almdps}}}}{69}{appendix.A}%
\contentsline {section}{\numberline {A.1}Preliminaries}{69}{section.A.1}%
\contentsline {section}{\numberline {A.2}Assumptions}{70}{section.A.2}%
\contentsline {section}{\numberline {A.3}The proof}{71}{section.A.3}%
