\babel@toc {english}{}\relax 
\babel@toc {catalan}{}\relax 
\contentsline {chapter}{{ Acknowledgements}}{v}{chapter*.1}%
\babel@toc {english}{}\relax 
\contentsline {chapter}{List of figures}{xiv}{chapter*.3}%
\contentsline {chapter}{List of tables}{xv}{chapter*.4}%
\contentsline {chapter}{\numberline {1}{{ Introduction}}}{1}{chapter.1}%
\contentsline {section}{\numberline {1.1}Thesis structure}{1}{section.1.1}%
\contentsline {section}{\numberline {1.2}Contributions}{2}{section.1.2}%
\contentsline {chapter}{\numberline {2}{{ Background}}}{3}{chapter.2}%
\contentsline {section}{\numberline {2.1}Markov decision processes}{3}{section.2.1}%
\contentsline {subsection}{\numberline {2.1.1}The discounted case}{5}{subsection.2.1.1}%
\contentsline {subsection}{\numberline {2.1.2}The average-reward case}{9}{subsection.2.1.2}%
\contentsline {section}{\numberline {2.2}Reinforcement learning}{12}{section.2.2}%
\contentsline {subsection}{\numberline {2.2.1}The discounted setting}{12}{subsection.2.2.1}%
\contentsline {subsection}{\numberline {2.2.2}The average-reward setting}{13}{subsection.2.2.2}%
\contentsline {section}{\numberline {2.3}Successor features}{14}{section.2.3}%
\contentsline {subsection}{\numberline {2.3.1}Combining policies in SF}{15}{subsection.2.3.1}%
\contentsline {section}{\numberline {2.4}Linearly-solvable Markov decision processes}{16}{section.2.4}%
\contentsline {subsection}{\numberline {2.4.1}First-exit linearly-solvable Markov decision processes}{16}{subsection.2.4.1}%
\contentsline {subsubsection}{Solving a first-exit LMDP}{18}{section*.9}%
\contentsline {subsection}{\numberline {2.4.2}Compositionality}{19}{subsection.2.4.2}%
\contentsline {subsection}{\numberline {2.4.3}Average-reward linearly-solvable Markov decision processes}{19}{subsection.2.4.3}%
\contentsline {subsubsection}{Solving an ALMDP}{20}{section*.10}%
\contentsline {subsection}{\numberline {2.4.4}Function approximation in LMDPs}{21}{subsection.2.4.4}%
\contentsline {section}{\numberline {2.5}Hierarchical reinforcement learning}{22}{section.2.5}%
\contentsline {subsection}{\numberline {2.5.1}The options framework}{23}{subsection.2.5.1}%
\contentsline {subsubsection}{Intra-task learning}{24}{section*.13}%
\contentsline {subsection}{\numberline {2.5.2}Optimality of HRL algorithms}{25}{subsection.2.5.2}%
\contentsline {section}{\numberline {2.6}Non-Markovian task specification}{25}{section.2.6}%
\contentsline {chapter}{\numberline {3}{{ Globally Optimal Hierarchical Reinforcement Learning for Linearly-solvable Markov Decision Processes}}}{27}{chapter.3}%
\contentsline {section}{\numberline {3.1}Introduction}{27}{section.3.1}%
\contentsline {section}{\numberline {3.2}Contributions}{28}{section.3.2}%
\contentsline {section}{\numberline {3.3}Related Work}{28}{section.3.3}%
\contentsline {section}{\numberline {3.4}Hierarchical LMDPs}{30}{section.3.4}%
\contentsline {subsection}{\numberline {3.4.1}Hierarchical Decomposition}{30}{subsection.3.4.1}%
\contentsline {subsection}{\numberline {3.4.2}Subtask Compositionality}{31}{subsection.3.4.2}%
\contentsline {subsection}{\numberline {3.4.3}Eigenvector Approach}{35}{subsection.3.4.3}%
\contentsline {subsection}{\numberline {3.4.4}Online and Intra-task Learning}{35}{subsection.3.4.4}%
\contentsline {subsection}{\numberline {3.4.5}Analysis}{37}{subsection.3.4.5}%
\contentsline {section}{\numberline {3.5}Experiments}{38}{section.3.5}%
\contentsline {subsection}{\numberline {3.5.1}N-room domain.}{39}{subsection.3.5.1}%
\contentsline {subsection}{\numberline {3.5.2}Taxi Domain.}{40}{subsection.3.5.2}%
\contentsline {section}{\numberline {3.6}Discussion and Conclusion}{41}{section.3.6}%
\contentsline {chapter}{\numberline {4}{{ Hierarchical Average-reward Linearly-solvable Markov Decision Pocesses}}}{43}{chapter.4}%
\contentsline {section}{\numberline {4.1}Introduction}{43}{section.4.1}%
\contentsline {section}{\numberline {4.2}Contributions}{44}{section.4.2}%
\contentsline {section}{\numberline {4.3}Related work}{44}{section.4.3}%
\contentsline {section}{\numberline {4.4}Alternative method for solving an ALMDP}{45}{section.4.4}%
\contentsline {section}{\numberline {4.5}Hierarchical Average-Reward LMDPs}{46}{section.4.5}%
\contentsline {subsection}{\numberline {4.5.1}Hierarchical Decomposition}{47}{subsection.4.5.1}%
\contentsline {subsection}{\numberline {4.5.2}Subtask Compositionality}{47}{subsection.4.5.2}%
\contentsline {subsection}{\numberline {4.5.3}Efficiency of the value representation}{48}{subsection.4.5.3}%
\contentsline {section}{\numberline {4.6}Algorithms}{49}{section.4.6}%
\contentsline {subsection}{\numberline {4.6.1}Eigenvector approach}{49}{subsection.4.6.1}%
\contentsline {subsection}{\numberline {4.6.2}Online hierarchical algorithm}{52}{subsection.4.6.2}%
\contentsline {section}{\numberline {4.7}Experiments}{53}{section.4.7}%
\contentsline {subsection}{\numberline {4.7.1}N-room domain}{54}{subsection.4.7.1}%
\contentsline {subsection}{\numberline {4.7.2}Taxi domain}{56}{subsection.4.7.2}%
\contentsline {section}{\numberline {4.8}Conclusion}{56}{section.4.8}%
\contentsline {chapter}{\numberline {5}{{ Compositionality with Successor Features via Policy Basis}}}{59}{chapter.5}%
\contentsline {section}{\numberline {5.1}Introduction}{59}{section.5.1}%
\contentsline {section}{\numberline {5.2}Contributions}{60}{section.5.2}%
\contentsline {section}{\numberline {5.3}Preliminaries}{61}{section.5.3}%
\contentsline {paragraph}{Propositional Logic}{61}{section*.25}%
\contentsline {paragraph}{Finite State Automaton}{61}{section*.26}%
\contentsline {paragraph}{Feature vectors}{61}{section*.28}%
\contentsline {paragraph}{Policy basis and convex coverage set}{62}{section*.29}%
\contentsline {section}{\numberline {5.4}Using Successor Features to Solve non-Markovian Reward Specifications}{63}{section.5.4}%
\contentsline {paragraph}{Example}{64}{section*.30}%
\contentsline {subsection}{\numberline {5.4.1}Algorithm}{64}{subsection.5.4.1}%
\contentsline {subsection}{\numberline {5.4.2}Analysis}{65}{subsection.5.4.2}%
\contentsline {section}{\numberline {5.5}Experiments}{66}{section.5.5}%
\contentsline {subsection}{\numberline {5.5.1}Environments and tasks}{67}{subsection.5.5.1}%
\contentsline {paragraph}{Office}{67}{section*.32}%
\contentsline {paragraph}{Delivery}{67}{section*.33}%
\contentsline {subsection}{\numberline {5.5.2}Baselines}{68}{subsection.5.5.2}%
\contentsline {subsection}{\numberline {5.5.3}Results}{69}{subsection.5.5.3}%
\contentsline {subsubsection}{A motivating example}{69}{section*.36}%
\contentsline {subsubsection}{Learning}{70}{section*.37}%
\contentsline {subsubsection}{Planning}{71}{section*.38}%
\contentsline {section}{\numberline {5.6}Related Work}{71}{section.5.6}%
\contentsline {section}{\numberline {5.7}Discussion and Conclusion}{73}{section.5.7}%
\contentsline {chapter}{\numberline {A}{{ Proof of Theorem~\ref {theo:almdps}}}}{83}{appendix.A}%
\contentsline {section}{\numberline {A.1}Preliminaries}{83}{section.A.1}%
\contentsline {section}{\numberline {A.2}Assumptions}{84}{section.A.2}%
\contentsline {section}{\numberline {A.3}The proof}{85}{section.A.3}%
