\babel@toc {english}{}\relax 
\babel@toc {catalan}{}\relax 
\addvspace {10\p@ }
\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Interaction loop in a Markov decision process. The agent observes current state $S_t$ and chooses $A_t\sim (\cdot \lvert S_t)$. The agent receives a feedback (reward) from the environment and a new state which from the point of view of the agent becomes the new current state. }}{4}{figure.caption.5}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces A communicating and unichain MDP. $\mathcal {S}\equiv \{s_0, s_1, s_2, s_3\}$ and there are up to 2 actions available in each state. $P_0(s)$ is $1$ if $s=s_0$ and $0$ otherwise.}}{10}{figure.caption.7}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces Modified version of Figure~\ref {fig:unichain} where transition between $s_2$ and $s_0$ is removed and self-loop actions are possible at $s_2$ and $s_3$ (colored in red).}}{10}{figure.caption.8}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces The interaction loop in a semi MDP where the underlying MDP is extended with options.}}{23}{figure.caption.12}%
\contentsline {figure}{\numberline {2.5}{\ignorespaces Example of a recursively optimal policy (left) and a hierarchically optimal one (right) in a . The area in green highlights the states in which policies disagree.}}{25}{figure.caption.14}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces a) A 4-room LMDP, with a terminal state $F$ and 8 other exit states; b) a single subtask with 5 terminal states $F,L,R,T,B$ that is equivalent to all 4 room subtasks. Rooms are numbered 1 through 4, left-to-right, then top-to-bottom, and exit state $1^B$ refers to the exit $B$ of room $1$, etc. Black squares represent exit states for which their value function is $z(s)=0$. The states must be present so that the passive dynamics at the interior states for all the equivalent}}{32}{figure.caption.15}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces a) A 5x5 grid of the Taxi LMDP, where there are 4 colored pickup/drop-off locations (which are one step away from the corresponding gridcell); b) the only existing subtask with exit states $E_1,E_2,E_3, \text {and} E_4$ which are one step away of the grid corners.}}{34}{figure.caption.16}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Results for $3\times 3$ rooms of size $5 \times 5$ (left); $5\times 5$ rooms of size $3 \times 3$ (center); $8 \times 8$ rooms of size $5\times 5$ (right).}}{40}{figure.caption.18}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces Results for $5 \times 5$ (left) and $10 \times 10$ (right) grids of Taxi domain.}}{41}{figure.caption.19}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces An example $4$-room ALMDP that shows the adaptation of the previously introduced N-room domain to the average-reward setting. }}{46}{figure.caption.20}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Error in $\setbox \z@ \hbox {\mathsurround \z@ $\textstyle \Gamma $}\mathaccent "0362{\Gamma }$ per iteration for Algorithm~\ref {alg:halmdps_eigenvector}.}}{54}{figure.caption.22}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Results in N-room when varying the number of rooms and the size of the rooms.}}{55}{figure.caption.23}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces Results for $5 \times 5$ (top) and $8 \times 8$ (bottom) grids of the Taxi domain.}}{57}{figure.caption.24}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces (a) Depiction of the Office environment. The propositional symbols are $\mathcal {P}=\{\text {{\color {black}{\fontfamily {mvs}\fontencoding {U}\fontseries {m}\fontshape {n}\selectfont \char 75}}\xspace }, \text {{\fontfamily {mvs}\fontencoding {U}\fontseries {m}\fontshape {n}\selectfont \char 66}}, o\}$ while the set of exit states is $\mathcal {E}=\{\text {{\color {black}{\fontfamily {mvs}\fontencoding {U}\fontseries {m}\fontshape {n}\selectfont \char 75}}\xspace }^1,\text {{\color {black}{\fontfamily {mvs}\fontencoding {U}\fontseries {m}\fontshape {n}\selectfont \char 75}}\xspace }^2, \text {{\fontfamily {mvs}\fontencoding {U}\fontseries {m}\fontshape {n}\selectfont \char 66}}^1,\text {{\fontfamily {mvs}\fontencoding {U}\fontseries {m}\fontshape {n}\selectfont \char 66}}^2, o^1, o^2\}$. The red and green paths show a suboptimal and optimal (resp.) trajectories for the task `get coffee and mail in any order, then go to an office location' whose FSA is represented in (b).}}{62}{figure.caption.27}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces Experimental results for learning (Delivery, top-left and Office, bottom-left) and compositionality (Delivery, top-right and Office, bottom-right). Results show the average performance and standard deviation over the three tasks and 5 seeds per task.}}{67}{figure.caption.31}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Depiction of the Office (a) and Delivery (b) environments, FSA task specification of the composite task in the Office domain and the FSA task specificiation of the sequential task in the Delivery domain (b). In (a) $\mathcal {P}=\{\text {{\color {black}{\fontfamily {mvs}\fontencoding {U}\fontseries {m}\fontshape {n}\selectfont \char 75}}\xspace }, \text {{\fontfamily {mvs}\fontencoding {U}\fontseries {m}\fontshape {n}\selectfont \char 66}}, o\}$ and $\mathcal {E}=\{\text {{\color {black}{\fontfamily {mvs}\fontencoding {U}\fontseries {m}\fontshape {n}\selectfont \char 75}}\xspace }^1,\text {{\color {black}{\fontfamily {mvs}\fontencoding {U}\fontseries {m}\fontshape {n}\selectfont \char 75}}\xspace }^2, \text {{\fontfamily {mvs}\fontencoding {U}\fontseries {m}\fontshape {n}\selectfont \char 66}}^1,\text {{\fontfamily {mvs}\fontencoding {U}\fontseries {m}\fontshape {n}\selectfont \char 66}}^2, o^1, o^2\}$. In (b), $\mathcal {E}=\mathcal {P}=\{A, B, C, H\}$.}}{68}{figure.caption.34}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Double slit environment (right) and FSA to achieve either of the goal states ({\color {blue}blue} or {\color {red}red}) (left.)}}{69}{figure.caption.35}%
\addvspace {10\p@ }
